env:
  scenarios:
    - 2_1_m2o
  envs_per_scenario: 8
  history_length: 4
  omnet:
    recv_len: 15  # number of features received from the OMNeT simulator
    size_of_data: 4  # size of float or uint32_t
    simulator_path: /swgwork/bfuhrer/projects/rlcc/new_simulator/MlnxCCSim #../simulator  # relative to reinforcement_learning dir ##FIXME changed path
    exe_path: /swgwork/bfuhrer/projects/rlcc/new_simulator/MlnxCCSim/bin/ccsim_release #../bin/ccsim_release  # relative to run_path ##FIXME
    config_path: /swgwork/bfuhrer/projects/rlcc/new_simulator/MlnxCCSim/sim/tests.ini #FIXME#ccsim.ini # relative to run_path
  reward: constrained
  default_port: 5555
  port_increment: 0

training:
  max_timesteps: 1000000
  learning_rate: 0.00025
  gradient_clip: .5
  replay_size: 1000000

agent:
  save_name: ''
  evaluate: False
  agent_type: PPO
  agent_features:
    - action
    - cnp_ratio
#    - relative_buffer_utilization
#    - switch_rate
#    - latency_mean_min_ratio
#    - bandwidth
#    - nack_indicator
#    - rtt_rate_signal
#    - nack_ratio
#    - latency_inflation
#    - requested_rate
  discount: .99
  linear_lr_decay: False
  activation_function: relu
  adpg:
    action_multiplier_dec: 0.2
    action_multiplier_inc: 0.2
    rollout_length: 1024
    architecture:
      - 32
      - 16
    use_rnn: null
    constraint: cnp
    beta: 1.5
    scale: 10.0
    target: 0.064
    loss_batch: 0 # use batch of agents in rollout instead of all agents
    max_batch_size: 8192
    max_step_size: 8192
    base_rtt: 8192.0
  supervised:
    action_multiplier_dec: 0.2
    action_multiplier_inc: 0.05
    batch_size: 128
    architecture:
      - 32
      - 16
  constrained:
    action_multiplier_dec: 0.2
    action_multiplier_inc: 0.05
    batch_size: 128
    rollout_length: 8
    learn_start: 1000
    actor_architecture:
      - 32
      - 16
    critic_architecture:
      - 32
      - 16
    model_architecture:
      - 32
      - 32
  ppo:
    action_multiplier_dec: 0.2
    action_multiplier_inc: 0.2
    discrete_actions: False
    action_weights:
      - 0.8
      - 0.95
      - 1
      - 1.05
      - 1.1
      - 1.2
    rollout_length: 20
    rollouts_per_batch: 4
    use_gae: False
    baseline_coeff: .5
    entropy_coeff: .01
    gae_tau: .95
    use_lstm: False
    actor_architecture:
      - 32
      - 16
    critic_architecture:
      - 32
      - 16
    params:
      ppo_ratio_clip: .2
      ppo_batch_size: 4
      ppo_optimization_epochs: 4
  dqn:
    architecture:
      - 32
      - 16
    action_weights:
      - 0.8
      - 0.95
      - 1
      - 1.05
      - 1.2
    batch_size: 32
    target_update_interval: 1024
    eps_start: 1.
    eps_end: 0.05
    eps_decay: 10000

logging:
  wandb: false  # Logging using weights and biases
  wandb_run_name: test  # Logging using weights and biases
  run_id: ''
  min_log_interval: 1024 # The minimum number of iterations before each log occurs
  num_tests_to_log: 1
  limit_flows: False
  limit_hosts: 3
  limit_qps: 3
