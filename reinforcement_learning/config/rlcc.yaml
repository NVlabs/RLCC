env:
  scenarios:
    - "4_1_m2o_l"
    - "8_1_m2o_l"
    - "32_1_m2o_l"
    - "4_16_a2a_l"
    - "4_8_a2a_l"
  envs_per_scenario: 1
  history_length: 2
  reward: adpg_reward
  port_increment: 10
  verbose: False
  restart_on_end: True
  multiprocess: False


training:
  max_num_updates: 1000
  learning_rate: 0.01
  gradient_clip: 0.5

agent:
  save_name: ''
  checkpoint: ''
  evaluate: False
  agent_type: ADPG
  agent_features:
    - action
    - adpg_reward
  discount: .99
  linear_lr_decay: False
  activation_function: relu
  adpg:
    action_multiplier_dec: 0.2
    action_multiplier_inc: 0.2
    rollout_length: 15
    architecture:
      - 12
      - 12
    use_rnn: null
    target: 0.064
    action_loss_coeff: 1
    beta: 1.5
    reward_loss_coeff: 1.0
    scale: 12.5
    bias: False
    loss_batch: 0 # use batch of agents in rollout instead of all agents
    max_batch_size: 8192
    max_step_size: 8000
    warmup_length: 1024
    warmup_updates: 50

logging:
  wandb: false  # Logging using weights and biases
  wandb_run_name: test  # Logging using weights and biases
  run_id: ''
  log_interval: 1024 # The minimum number of iterations before each log occurs
  limit_flows: False
  limit_hosts: 2
  limit_qps: 2
