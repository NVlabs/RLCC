env:
  scenarios:
    - 2_to_1
    - 4_to_1 
    - 8_to_1 
    - 2_to_2 
    - 4_to_4
    - 8_to_8
    - 32_to_1
    - 64_to_1
    - 128_to_1
    - 256_to_1
    - 512_to_1
    - 2_4_a2a
    - 4_4_a2a
    - 8_16_a2a
  envs_per_scenario: 1
  history_length: 1
  omnet:
    simulator_path: /swgwork/bfuhrer/projects/rlcc/new_simulator/rl-cc-demo//NVIDIACCSim/sim  # relative to reinforcement_learning dir 
    exe_path: /swgwork/bfuhrer/projects/rlcc/new_simulator/rl-cc-demo/NVIDIACCSim/bin/ccsim_release  #../bin/ccsim_release  # relative to run_path ##FIXME
    config_path: /swgwork/bfuhrer/projects/rlcc/new_simulator/rl-cc-demo/NVIDIACCSim/sim/omnetpp.ini #FIXME#ccsim.ini # relative to run_path
  reward: rtt_reward
  port_increment: 0
  save_path: '/swgwork/bfuhrer/projects/rlcc/new_simulator/rl-cc-demo/reinforcement_learning/saved_models/'

training:
  max_num_updates: 250
  learning_rate: 0.01
  gradient_clip: 0.5

agent:
  save_name: ''
  checkpoint: ''
  evaluate: False
  agent_type: ADPG
  agent_features:
    - action
    - rtt_reward
  discount: .99
  linear_lr_decay: False
  activation_function: relu
  adpg:
    action_multiplier_dec: 0.2
    action_multiplier_inc: 0.2
    rollout_length: 1024
    architecture:
      - 12
    use_rnn: null
    target: 0.8
    action_loss_coeff: 1
    beta: 1.5
    loss_scale: 10.0
    scale: 10.0
    bias: False
    constraint: buffer
    loss_batch: 0 # use batch of agents in rollout instead of all agents
    warmup_updates: 20
    warmup_length: 1024
    max_batch_size: 8192
    max_step_size: 8192
    base_rtt: 8192.0
logging:
  wandb: False  # Logging using weights and biases
  wandb_run_name: unnamed  # Logging using weights and biases
  run_id: ''
  min_log_interval: 16024 # The minimum number of iterations before each log occurs
  num_tests_to_log: 1
  limit_flows: False
  limit_hosts: 2
  limit_qps: 3
