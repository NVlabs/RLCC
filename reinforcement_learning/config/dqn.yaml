env:
  scenarios:
    - "4_1_m2o_l"
    - "4_16_m2o_l"
    - "8_1_m2o_l"
    - "4_16_a2a_l"
    - "8_8_a2a_l"
    - "8_16_a2a_l"
    - "32_1_m2o_l"
  envs_per_scenario: 1
  history_length: 2
  omnet:
    simulator_path: ../nv_ccsim/sim  # relative to reinforcement_learning dir
    exe_path: ../bin/ccsim_release  #../bin/ccsim_release  # relative to run_path
    config_path: omnetpp.ini
  reward: general
  port_increment: 10
  save_path: '../../reinforcement_learning/output/'  # relative path to NVIDIACCSimSetup/sim/
  verbose: False
  restart_on_end: True
  multiprocess: False

training:
  max_num_updates: 1000
  learning_rate: 0.00025
  gradient_clip: .5
  replay_size: 10000

agent:
  save_name: ''
  evaluate: False
  agent_type: DQN
  agent_features:
    - action
    - cnp_ratio
    - nack_ratio
#    - bandwidth
#    - rtt_inflation
#    - cur_rate
  discount: .99
  linear_lr_decay: False
  dqn:
    target_update_interval: 10000
    eps_start: 1.
    eps_end: 0.05
    eps_decay: 100000
    batch_size: 256
    activation_function: relu
    bias: false
    use_rnn: null
    action_weights:
      - 0.8
      - 0.95
      - 1
      - 1.05
      - 1.1
      - 1.2
    architecture:
      - 32
      - 16
  adpg:
    beta: 1.5
    target: 0.064
    scale: 12.5

logging:
  wandb: false  # Logging using weights and biases
  wandb_run_name: test  # Logging using weights and biases
  run_id: ''
  min_log_interval: 1024 # The minimum number of iterations before each log occurs
  num_tests_to_log: 1
  limit_flows: False
  limit_hosts: 3
  limit_qps: 3
