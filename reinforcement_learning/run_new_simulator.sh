
python run.py --envs_per_scenario 1 --wandb rlcc-new_simulator --wandb_run_name rtt_12_h_tgt_0.96_f_2_max_0_rl_1024_p_1_act_0.2_alc_1_rlc_5_lr_0.01_envs_1_qp_8k_bst_hl_2_no_lstm_lat_3000_v5 --learning_rate 0.01 --agent ADPG  --config rtt_adpg --architecture 12 --max_num_updates 1000 --action_multiplier_inc 0.2 --action_multiplier_dec 0.2 --agent_features action rtt_reward --target 0.96 --factor 2 --max_factor 0 --rollout_length 1024 --action_loss_coeff 1.0 --loss_scale 5.0 --scenarios  2_1_qp 4_1_qp 8_1_qp 4_2_a2a_qp 4_8_a2a_qp 8_16_a2a_qp 32_1_qp --limit_flows --use_lstm False --history_length 2 --port_increment 50
python run.py --envs_per_scenario 1 --wandb rlcc-new_simulator --wandb_run_name rtt_12_h_tgt_0.8_f_12.5_max_1.5_rl_1024_p_1_act_0.2_alc_1_rlc_5_lr_0.01_envs_1_qp_8k_bst_hl_2_no_lstm_lat_3000_v5 --learning_rate 0.01 --agent ADPG  --config rtt_adpg --architecture 12 --max_num_updates 1000 --action_multiplier_inc 0.2 --action_multiplier_dec 0.2 --agent_features action rtt_reward --target 0.8 --factor 12.5 --max_factor 1.5 --rollout_length 1024 --action_loss_coeff 1.0 --loss_scale 5.0 --scenarios  2_1_qp 4_1_qp 8_1_qp 4_2_a2a_qp 4_8_a2a_qp 8_16_a2a_qp 32_1_qp --limit_flows --use_lstm False --history_length 2 --port_increment 50


python run.py --envs_per_scenario 1 --wandb rlcc-new_simulator --wandb_run_name rtt_12_h_tgt_25_f_100_max_0_rl_1024_p_1_act_0.2_alc_1_rlc_5_lr_0.01_envs_1_qp_8k_bst_hl_2_no_lstm_lat_3000_v5 --learning_rate 0.01 --agent ADPG  --config rtt_adpg --architecture 12 --max_num_updates 1000 --action_multiplier_inc 0.2 --action_multiplier_dec 0.2 --agent_features action rtt_reward --target 25 --factor 100 --max_factor 0 --rollout_length 1024 --action_loss_coeff 1.0 --loss_scale 5.0 --scenarios  2_1_qp 4_1_qp 8_1_qp 4_2_a2a_qp 4_8_a2a_qp 8_16_a2a_qp 32_1_qp --use_lstm False --history_length 2 --port_increment 50

"""
Can't get similar results to previous -> debugging
"""
python run.py --envs_per_scenario 1 --wandb rlcc-new_simulator --wandb_run_name rtt_12_h_tgt_0.8_f_12.5_max_1.5_rl_1024_p_1_act_0.2_alc_1_rlc_5_lr_0.01_envs_1_qp_8k_bst_hl_2_no_lstm --learning_rate 0.01 --agent ADPG  --config rtt_adpg --architecture 12 --max_num_updates 1000 --action_multiplier_inc 0.2 --action_multiplier_dec 0.2 --agent_features action rtt_reward --target 0.8 --factor 12.5 --max_factor 1.5 --rollout_length 1024 --action_loss_coeff 1.0 --loss_scale 5.0 --scenarios  2_1_qp 4_1_qp 8_1_qp 4_2_a2a_qp 4_8_a2a_qp 8_16_a2a_qp 32_1_qp --use_lstm False --history_length 2 --port_increment 50
# burst size 2
python run.py --envs_per_scenario 1 --wandb rlcc-new_simulator --wandb_run_name rtt_12_h_tgt_0.8_f_12.5_max_1.5_rl_1024_p_1_act_0.2_alc_1_rlc_5_lr_0.01_envs_1_qp_2k_bst_hl_2_no_lstm --learning_rate 0.01 --agent ADPG  --config rtt_adpg --architecture 12 --max_num_updates 1000 --action_multiplier_inc 0.2 --action_multiplier_dec 0.2 --agent_features action rtt_reward --target 0.8 --factor 12.5 --max_factor 1.5 --rollout_length 1024 --action_loss_coeff 1.0 --loss_scale 5.0 --scenarios  2_1_qp 4_1_qp 8_1_qp 4_2_a2a_qp 4_8_a2a_qp 8_16_a2a_qp 32_1_qp --use_lstm False --history_length 2 --port_increment 50

python run.py --envs_per_scenario 1 --wandb rlcc-new_simulator --wandb_run_name rtt_12_h_tgt_0.8_f_12.5_max_1.5_rl_20_p_1_act_0.2_alc_1_rlc_5_lr_0.01_envs_1_qp_2k_bst_hl_2_leakyrelu --learning_rate 0.01 --agent ADPG  --config rtt_adpg --architecture 12 --max_num_updates 1000 --action_multiplier_inc 0.2 --action_multiplier_dec 0.2 --agent_features action rtt_reward --target 0.8 --factor 12.5 --max_factor 1.5 --rollout_length 20 --action_loss_coeff 1.0 --loss_scale 5.0 --scenarios  2_1_qp 4_1_qp 8_1_qp 4_2_a2a_qp 4_8_a2a_qp 8_16_a2a_qp 32_1_qp --use_lstm False --history_length 2 --port_increment 50

# sanity check only m2o
python run.py --envs_per_scenario 1 --wandb rlcc-new_simulator --wandb_run_name rtt_12_h_tgt_0.8_f_12.5_max_1.5_rl_1024_p_1_act_0.2_alc_1_rlc_5_lr_0.01_envs_1_qp_2k_bst_hl_2_leakyrelu_m2o --learning_rate 0.01 --agent ADPG  --config rtt_adpg --architecture 12 --max_num_updates 1000 --action_multiplier_inc 0.2 --action_multiplier_dec 0.2 --agent_features action rtt_reward --target 0.8 --factor 12.5 --max_factor 1.5 --rollout_length 1024 --action_loss_coeff 1.0 --loss_scale 5.0 --scenarios  2_1_qp 4_1_qp 8_1_qp 32_1_qp --use_lstm False --history_length 2 --port_increment 50
# with a2a
python run.py --envs_per_scenario 1 --wandb rlcc-new_simulator --wandb_run_name rtt_12_h_tgt_0.8_f_12.5_max_1.5_rl_1024_p_1_act_0.2_alc_1_rlc_5_lr_0.01_envs_1_qp_2k_bst_hl_2_leakyrelu --learning_rate 0.01 --agent ADPG  --config rtt_adpg --architecture 12 --max_num_updates 1000 --action_multiplier_inc 0.2 --action_multiplier_dec 0.2 --agent_features action rtt_reward --target 0.8 --factor 12.5 --max_factor 1.5 --rollout_length 1024 --action_loss_coeff 1.0 --loss_scale 5.0 --scenarios  2_1_qp 4_1_qp 8_1_qp 32_1_qp 2_4_a2a_qp 4_8_a2a_qp 8_16_a2a_qp --use_lstm False --history_length 2 --port_increment 50
#test only a2a
python run.py --envs_per_scenario 1 --wandb rlcc-new_simulator --wandb_run_name test --learning_rate 0.01 --agent ADPG  --config rtt_adpg --architecture 12 --max_num_updates 1000 --action_multiplier_inc 0.2 --action_multiplier_dec 0.2 --agent_features action rtt_reward --target 0.8 --factor 12.5 --max_factor 1.5 --rollout_length 1024 --action_loss_coeff 1.0 --loss_scale 5.0 --scenarios  8_16_a2a_qp --use_lstm False --history_length 2 --port_increment 50

"""
Still problem with alltoall
"""
python run.py --envs_per_scenario 1 --wandb rlcc-new_simulator --wandb_run_name rtt_12_h_tgt_1.2_f_12.5_max_1.5_rl_1024_p_1_act_0.2_alc_1_rlc_5_lr_0.01_envs_1_qp_2k_bst_hl_2_leakyrelu --learning_rate 0.01 --agent ADPG  --config rtt_adpg --architecture 12 --max_num_updates 1000 --action_multiplier_inc 0.2 --action_multiplier_dec 0.2 --agent_features action rtt_reward --target 1.2 --factor 12.5 --max_factor 1.5 --rollout_length 1024 --action_loss_coeff 1.0 --loss_scale 5.0 --scenarios  2_1_qp 4_1_qp 8_1_qp 32_1_qp 2_4_a2a_qp 4_8_a2a_qp 8_16_a2a_qp --use_lstm False --history_length 2 --port_increment 50

"""
It actually seems that the max factor is the issue
"""
python run.py --envs_per_scenario 1 --wandb rlcc-new_simulator --wandb_run_name rtt_12_h_tgt_0.8_f_12.5_max_1_rl_1024_p_1_act_0.2_alc_1_rlc_5_lr_0.01_envs_1_qp_2k_bst_hl_2_leakyrelu --learning_rate 0.01 --agent ADPG  --config rtt_adpg --architecture 12 --max_num_updates 1000 --action_multiplier_inc 0.2 --action_multiplier_dec 0.2 --agent_features action rtt_reward --target 0.8 --factor 12.5 --max_factor 1 --rollout_length 1024 --action_loss_coeff 1.0 --loss_scale 5.0 --scenarios  2_1_qp 4_1_qp 8_1_qp 32_1_qp 2_4_a2a_qp 4_8_a2a_qp 8_16_a2a_qp --use_lstm False --history_length 2 --port_increment 50
python run.py --envs_per_scenario 1 --wandb rlcc-new_simulator --wandb_run_name rtt_12_h_tgt_0.8_f_12.5_max_1.5_rl_1024_p_1_act_0.2_alc_1_rlc_5_lr_0.01_envs_1_qp_2k_bst_hl_2_leakyrelu_nonuniform --learning_rate 0.01 --agent ADPG  --config rtt_adpg --architecture 12 --max_num_updates 1000 --action_multiplier_inc 0.2 --action_multiplier_dec 0.2 --agent_features action rtt_reward --target 0.8 --factor 12.5 --max_factor 1.5 --rollout_length 1024 --action_loss_coeff 1.0 --loss_scale 5.0 --scenarios  2_1_qp 4_1_qp 8_1_qp 32_1_qp 2_4_a2a_qp 4_8_a2a_qp 8_16_a2a_qp --use_lstm False --history_length 2 --port_increment 50

"""
Training new simulator with settings from newest python version used in the distillation repo 31/03/2022
"""
python run.py --envs_per_scenario 3 --wandb rlcc-new_simulator --wandb_run_name rtt_12_h_tgt_0.8_f_0.0015258789_max_12288_p_1_rl_5_act_0.2_alc_1_sl_10_lr_0.01_envs_3_qp_2k_bst_hl_2_relu_x_smplea2a_wmpup_150_wmplen_512_lossbatch_50 --learning_rate 0.01 --agent ADPG --config rtt_adpg --architecture 12 --max_num_updates 1000 --action_multiplier_inc 0.2 --action_multiplier_dec 0.2 --agent_features action rtt_reward --target 0.8 --factor 0.0015258789 --power 1 --max_factor 12288 --rollout_length 5 --action_loss_coeff 1 --loss_scale 10.0 --scenarios 2_1_qp 4_1_qp 8_1_qp 2_4_a2a_qp 4_8_a2a_qp 8_16_a2a_qp 32_1_qp 8_16_a2a_qp 16_32_a2a_qp --limit_flows --use_lstm False --history_length 2 --leaky_relu_coeff 0.0 --warmup_updates 150 --warmup_length 512 --loss_batch 50 --port_increment 50
"""
It seems that 2 Hosts 4 QPs A2A can't achieve a high enough RTT value to exceed MAX_FACTOR --> we can't train on it
"""
python run.py --envs_per_scenario 3 --wandb rlcc-new_simulator --wandb_run_name rtt_12_h_tgt_0.8_f_0.0015258789_max_12288_p_1_rl_5_act_0.2_alc_1_sl_10_lr_0.01_envs_3_qp_2k_bst_hl_2_relu_x_smplea2a_wmpup_150_wmplen_512_lossbatch_50_no_2_4a2a --learning_rate 0.01 --agent ADPG --config rtt_adpg --architecture 12 --max_num_updates 1000 --action_multiplier_inc 0.2 --action_multiplier_dec 0.2 --agent_features action rtt_reward --target 0.8 --factor 0.0015258789 --power 1 --max_factor 12288 --rollout_length 5 --action_loss_coeff 1 --loss_scale 10.0 --scenarios 2_1_qp 4_1_qp 8_1_qp 4_8_a2a_qp 8_16_a2a_qp 32_1_qp 8_16_a2a_qp 16_32_a2a_qp --limit_flows --use_lstm False --history_length 2 --leaky_relu_coeff 0.0 --warmup_updates 150 --warmup_length 512 --loss_batch 50 --port_increment 50


python run.py --envs_per_scenario 3 --wandb rlcc-new_simulator --wandb_run_name rtt_12_h_tgt_0.8_f_0.0015258789_max_12288_p_1_rl_5_act_0.2_alc_1_sl_10_lr_0.01_envs_3_qp_2k_bst_hl_2_relu_x_smplea2a_wmpup_150_wmplen_512_no_2_4a2a --learning_rate 0.01 --agent ADPG --config rtt_adpg --architecture 12 --max_num_updates 1000 --action_multiplier_inc 0.2 --action_multiplier_dec 0.2 --agent_features action rtt_reward --target 0.8 --factor 0.0015258789 --power 1 --max_factor 12288 --rollout_length 5 --action_loss_coeff 1 --loss_scale 10.0 --scenarios 2_1_qp 4_1_qp 8_1_qp 4_8_a2a_qp 8_16_a2a_qp 32_1_qp 8_16_a2a_qp 16_32_a2a_qp --limit_flows --use_lstm False --history_length 2 --leaky_relu_coeff 0.0 --warmup_updates 150 --warmup_length 512 --port_increment 50
python run.py --envs_per_scenario 1 --wandb rlcc-new_simulator --wandb_run_name rtt_12_h_tgt_0.8_f_0.0015258789_max_12288_p_1_rl_5_act_0.2_alc_1_sl_10_lr_0.01_envs_1_qp_4k_bst_hl_2_relu_x_smplea2a_wmpup_150_wmplen_512_no_2_4a2a --learning_rate 0.01 --agent ADPG --config rtt_adpg --architecture 12 --max_num_updates 1000 --action_multiplier_inc 0.2 --action_multiplier_dec 0.2 --agent_features action rtt_reward --target 0.8 --factor 0.0015258789 --power 1 --max_factor 12288 --rollout_length 5 --action_loss_coeff 1 --loss_scale 10.0 --scenarios 2_1_qp 4_1_qp 8_1_qp 4_8_a2a_qp 8_16_a2a_qp 32_1_qp 8_16_a2a_qp 16_32_a2a_qp --limit_flows --use_lstm False --history_length 2 --leaky_relu_coeff 0.0 --warmup_updates 150 --warmup_length 512 --port_increment 50

"""
Still seems jittery -> reduce reward loss coeff and changed config to plot more hosts as host 0 doesn't exist for M2O --> need to understand why it is jittery
"""
python run.py --envs_per_scenario 1 --wandb rlcc-new_simulator --wandb_run_name rtt_12_h_tgt_0.8_f_0.0015258789_max_12288_p_1_rl_5_act_0.2_alc_1_sl_2.5_lr_0.01_envs_1_qp_4k_bst_hl_2_relu_x_smplea2a_wmpup_150_wmplen_512_no_2_4a2a --learning_rate 0.01 --agent ADPG --config rtt_adpg --architecture 12 --max_num_updates 1000 --action_multiplier_inc 0.2 --action_multiplier_dec 0.2 --agent_features action rtt_reward --target 0.8 --factor 0.0015258789 --power 1 --max_factor 12288 --rollout_length 5 --action_loss_coeff 1 --loss_scale 2.5 --scenarios 2_1_qp 4_1_qp 8_1_qp 4_8_a2a_qp 8_16_a2a_qp 32_1_qp 8_16_a2a_qp 16_32_a2a_qp --limit_flows --use_lstm False --history_length 2 --leaky_relu_coeff 0.0 --warmup_updates 150 --warmup_length 512 --port_increment 50
"""
No gradient at the beginning -> trying to move to leaky relu
"""
python3 run.py --envs_per_scenario 1 --wandb rlcc-new_simulator --wandb_run_name rtt_12_h_tgt_0.8_f_0.0015258789_max_12288_p_1_rl_5_act_0.2_alc_1_sl_2.5_lr_0.01_envs_1_qp_4k_bst_hl_2_lrelu_0.01_smplea2a_wmpup_150_wmplen_512_no_2_4a2a --learning_rate 0.01 --agent ADPG --config rtt_adpg --architecture 12 --max_num_updates 1000 --action_multiplier_inc 0.2 --action_multiplier_dec 0.2 --agent_features action rtt_reward --target 0.8 --factor 0.0015258789 --power 1 --max_factor 12288 --rollout_length 5 --action_loss_coeff 1 --loss_scale 2.5 --scenarios 2_1_qp 4_1_qp 8_1_qp 4_8_a2a_qp 8_16_a2a_qp 32_1_qp 16_32_a2a_qp --limit_flows --use_lstm False --history_length 2 --leaky_relu_coeff 0.01 --warmup_updates 150 --warmup_length 512 --port_increment 50
python3 run.py --envs_per_scenario 1 --wandb rlcc-new_simulator --wandb_run_name rtt_12_h_tgt_0.8_f_0.0015258789_max_12288_p_1_rl_2_act_0.2_alc_1_sl_2.5_lr_0.01_envs_1_qp_4k_bst_hl_2_lrelu_0.01_smplea2a_wmpup_150_wmplen_512_no_2_4a2a --learning_rate 0.01 --agent ADPG --config rtt_adpg --architecture 12 --max_num_updates 1000 --action_multiplier_inc 0.2 --action_multiplier_dec 0.2 --agent_features action rtt_reward --target 0.8 --factor 0.0015258789 --power 1 --max_factor 12288 --rollout_length 2 --action_loss_coeff 1 --loss_scale 2.5 --scenarios 2_1_qp 4_1_qp 8_1_qp 4_8_a2a_qp 8_16_a2a_qp 32_1_qp 16_32_a2a_qp --limit_flows --use_lstm False --history_length 2 --leaky_relu_coeff 0.01 --warmup_updates 150 --warmup_length 512 --port_increment 50

"""
Seems that 4_8 a2a doesn't really need CC -> removing it and trying old simulator runs on new simulator
1) only qp
2) qp + ip but several agents from IP
3) telemetry
"""
#1
python3 run.py --envs_per_scenario 1 --wandb rlcc-new_simulator --wandb_run_name rtt_12_h_tgt_0.8_f_0.0015258789_max_12288_p_1_rl_3_act_0.2_alc_1_sl_2.5_lr_0.01_envs_1_qp_4k_bst_hl_2_relu_wmpup_50_wmplen_1024 --learning_rate 0.01 --agent ADPG --config rtt_adpg --architecture 12 --max_num_updates 1000 --action_multiplier_inc 0.2 --action_multiplier_dec 0.2 --agent_features action rtt_reward --target 0.8 --factor 0.0015258789 --power 1 --max_factor 12288 --rollout_length 3 --action_loss_coeff 1 --loss_scale 2.5 --scenarios 2_1_qp 4_1_qp 8_1_qp 4_16_a2a_qp 8_8_a2a_qp 8_16_a2a_qp 32_1_qp  --limit_flows --use_lstm False --history_length 2 --leaky_relu_coeff 0.0 --warmup_updates 50 --warmup_length 1024 --port_increment 50
#2
python3 run.py --envs_per_scenario 1 --wandb rlcc-new_simulator --wandb_run_name rtt_12_h_tgt_0.8_f_0.0015258789_max_12288_p_1_rl_3_act_0.2_alc_1_sl_2.5_lr_0.01_envs_1_qp_ip_4k_bst_hl_2_relu_wmpup_50_wmplen_1024 --learning_rate 0.01 --agent ADPG --config rtt_adpg --architecture 12 --max_num_updates 1000 --action_multiplier_inc 0.2 --action_multiplier_dec 0.2 --agent_features action rtt_reward --target 0.8 --factor 0.0015258789 --power 1 --max_factor 12288 --rollout_length 3 --action_loss_coeff 1 --loss_scale 2.5 --scenarios 2_1_qp 4_1_qp 8_1_qp 4_16_a2a_qp 8_8_a2a_qp 8_16_a2a_qp 32_1_qp 8_1024_ip  --limit_flows --use_lstm False --history_length 2 --leaky_relu_coeff 0.0 --warmup_updates 50 --warmup_length 1024 --port_increment 50
#3
python3 run.py --envs_per_scenario 1 --wandb rlcc-new_simulator --wandb_run_name tele_12_h_tgt_0.8_f_2.44_max_10_qnorm_80_p_1_rl_5_act_0.2_alc_1_sl_1_lr_0.01_envs_1_qp_4k_bst_lstm_relu_wmpup_50_wmplen_1024 --learning_rate 0.01 --agent ADPG --config telemetry_adpg --architecture 12 --max_num_updates 1000 --action_multiplier_inc 0.2 --action_multiplier_dec 0.2 --agent_features action qlength_reward --target 0.8 --factor 1.22 --power 1 --max_factor 5 --qnorm 40  --rollout_length 5 --action_loss_coeff 1 --loss_scale 1.0 --scenarios 2_1_qp_t 4_1_qp_t 8_1_qp_t 4_16_a2a_qp_t 8_8_a2a_qp_t 8_16_a2a_qp_t 32_1_qp_t  --limit_flows --use_lstm True --history_length 1 --leaky_relu_coeff 0.0 --warmup_updates 50 --warmup_length 1024 --port_increment 50
#4 - max batch size = 512
python3 run.py --envs_per_scenario 1 --wandb rlcc-new_simulator --wandb_run_name rtt_12_h_tgt_0.8_f_0.0015258789_max_12288_p_1_rl_3_act_0.2_alc_1_sl_2.5_lr_0.01_envs_1_qp_4k_bst_hl_2_relu_wmpup_50_wmplen_1024_mbatch_512 --learning_rate 0.01 --agent ADPG --config rtt_adpg --architecture 12 --max_num_updates 1000 --action_multiplier_inc 0.2 --action_multiplier_dec 0.2 --agent_features action rtt_reward --target 0.8 --factor 0.0015258789 --power 1 --max_factor 12288 --rollout_length 3 --action_loss_coeff 1 --loss_scale 2.5 --scenarios 2_1_qp 4_1_qp 8_1_qp 4_16_a2a_qp 8_8_a2a_qp 8_16_a2a_qp 32_1_qp  --limit_flows --use_lstm False --history_length 2 --leaky_relu_coeff 0.0 --warmup_updates 50 --warmup_length 1024 --port_increment 50 --max_batch_size 512
# 5 added discount factor -> batch=size = 8192 and gamma = 0.99
python3 run.py --envs_per_scenario 1 --wandb rlcc-new_simulator --wandb_run_name rtt_12_h_tgt_0.8_f_0.0015258789_max_12288_p_1_rl_3_act_0.2_alc_1_sl_2.5_lr_0.01_envs_1_qp_4k_bst_hl_2_relu_wmpup_50_wmplen_1024_mbatch_8192_g_0.99 --learning_rate 0.01 --agent ADPG --config rtt_adpg --architecture 12 --max_num_updates 1000 --action_multiplier_inc 0.2 --action_multiplier_dec 0.2 --agent_features action rtt_reward --target 0.8 --factor 0.0015258789 --power 1 --max_factor 12288 --rollout_length 3 --action_loss_coeff 1 --loss_scale 2.5 --scenarios 2_1_qp 4_1_qp 8_1_qp 4_16_a2a_qp 8_8_a2a_qp 8_16_a2a_qp 32_1_qp  --limit_flows --use_lstm False --history_length 2 --leaky_relu_coeff 0.0 --warmup_updates 50 --warmup_length 1024 --port_increment 50 --max_batch_size 8192 --discount 0.99
"""
#5 limit seems too high -> trying again with max step size 8192 + see effects of slightly larger target
"""
python3 run.py --envs_per_scenario 1 --wandb rlcc-new_simulator --wandb_run_name rtt_12_h_tgt_0.9_f_0.0015258789_max_12288_p_1_rl_3_act_0.2_alc_1_sl_2.5_lr_0.01_envs_1_qp_4k_bst_hl_2_relu_wmpup_50_wmplen_1024_mbatch_8192_msz_8192_g_0.99 --learning_rate 0.01 --agent ADPG --config rtt_adpg --architecture 12 --max_num_updates 1000 --action_multiplier_inc 0.2 --action_multiplier_dec 0.2 --agent_features action rtt_reward --target 0.9 --factor 0.0015258789 --power 1 --max_factor 12288 --rollout_length 3 --action_loss_coeff 1 --loss_scale 2.5 --scenarios 2_1_qp 4_1_qp 8_1_qp 4_16_a2a_qp 8_8_a2a_qp 8_16_a2a_qp 32_1_qp  --limit_flows --use_lstm False --history_length 2 --leaky_relu_coeff 0.0 --warmup_updates 50 --warmup_length 1024 --port_increment 50 --max_batch_size 8192 --discount 0.99
# add lstm to see if it makes difference in behavior
python3 run.py --envs_per_scenario 1 --wandb rlcc-new_simulator --wandb_run_name rtt_12_h_tgt_0.9_f_0.0015258789_max_12288_p_1_rl_3_act_0.2_alc_1_sl_2.5_lr_0.01_envs_1_qp_4k_bst_lstm_relu_wmpup_50_wmplen_1024_mbatch_8192_msz_8192_g_0.99 --learning_rate 0.01 --agent ADPG --config rtt_adpg --architecture 12 --max_num_updates 1000 --action_multiplier_inc 0.2 --action_multiplier_dec 0.2 --agent_features action rtt_reward --target 0.9 --factor 0.0015258789 --power 1 --max_factor 12288 --rollout_length 3 --action_loss_coeff 1 --loss_scale 2.5 --scenarios 2_1_qp 4_1_qp 8_1_qp 4_16_a2a_qp 8_8_a2a_qp 8_16_a2a_qp 32_1_qp  --limit_flows --use_lstm True --history_length 1 --leaky_relu_coeff 0.0 --warmup_updates 50 --warmup_length 1024 --port_increment 50 --max_batch_size 8192 --discount 0.99

"""
The 0.9 with gamma (discount seems to work fine), LSTM reacts faster.
Still issue with unfairness for IP mode -> trying to add time since last update as input
"""
python3 run.py --envs_per_scenario 1 --wandb rlcc-new_simulator --wandb_run_name rtt_12_h_tgt_0.8_f_0.0015258789_max_12288_p_1_rl_3_act_0.2_alc_1_sl_2.5_lr_0.01_envs_1_qp_ip_4k_bst_hl_2_relu_wmpup_50_wmplen_1024_mbatch_8192_msz_8192_g_0.99_updatedelay --learning_rate 0.01 --agent ADPG --config rtt_adpg --architecture 12 --max_num_updates 1000 --action_multiplier_inc 0.2 --action_multiplier_dec 0.2 --agent_features action rtt_reward update_delay --target 0.8 --factor 0.0015258789 --power 1 --max_factor 12288 --rollout_length 3 --action_loss_coeff 1 --loss_scale 2.5 --scenarios 2_1_qp 4_1_qp 8_1_qp 4_16_a2a_qp 8_8_a2a_qp 8_16_a2a_qp 32_1_qp 4_512_ip 4_1024_ip 8_1024_ip  --limit_flows --use_lstm False --history_length 2 --leaky_relu_coeff 0.0 --warmup_updates 50 --warmup_length 1024 --port_increment 50 --max_batch_size 8192 --discount 0.99
python3 run.py --envs_per_scenario 1 --wandb rlcc-new_simulator --wandb_run_name rtt_12_h_tgt_0.8_f_0.0015258789_max_12288_p_1_rl_3_act_0.2_alc_1_sl_2.5_lr_0.01_envs_1_qp_ip_4k_bst_hl_2_relu_wmpup_50_wmplen_1024_mbatch_8192_msz_8192_g_0.99_updatedelay_fxbugin_szv2 --learning_rate 0.01 --agent ADPG --config rtt_adpg --architecture 12 --max_num_updates 1000 --action_multiplier_inc 0.2 --action_multiplier_dec 0.2 --agent_features action rtt_reward update_delay --target 0.8 --factor 0.0015258789 --power 1 --max_factor 12288 --rollout_length 3 --action_loss_coeff 1 --loss_scale 2.5 --scenarios 2_1_qp 4_1_qp 8_1_qp 4_16_a2a_qp 8_8_a2a_qp 8_16_a2a_qp 32_1_qp 4_512_ip 4_1024_ip 8_1024_ip  --limit_flows --use_lstm False --history_length 2 --leaky_relu_coeff 0.0 --warmup_updates 50 --warmup_length 1024 --port_increment 50 --max_batch_size 8192 --discount 0.99 --max_step_size 8192

"""
Re-trying telemetry
"""
python3 run.py --envs_per_scenario 1 --wandb rlcc-new_simulator --wandb_run_name tele_12_h_tgt_0.8_f_2.44_max_1_qnorm_80_p_1_rl_5_act_0.2_alc_1_sl_1_lr_0.01_envs_1_qp_4k_bst_lstm_lrelu0.01_wmpup_50_wmplen_1024_mbatch_8192_msz_4000_g_0.99 --learning_rate 0.01 --agent ADPG --config telemetry_adpg --architecture 12 --max_num_updates 1000 --action_multiplier_inc 0.2 --action_multiplier_dec 0.2 --agent_features action qlength_reward --target 0.8 --factor 1.22 --power 1 --max_factor 1 --qnorm 80  --rollout_length 5 --action_loss_coeff 1 --loss_scale 1.0 --scenarios 2_1_qp_t 4_1_qp_t 8_1_qp_t 4_16_a2a_qp_t 8_8_a2a_qp_t 8_16_a2a_qp_t 32_1_qp_t  --limit_flows --use_lstm True --history_length 1 --leaky_relu_coeff 0.01 --warmup_updates 50 --warmup_length 1024 --max_batch_size 8192 --discount 0.99 --max_step_size 4000 --port_increment 50
# issues with gradient -> trying to remove tanh from loss calculation
python3 run.py --envs_per_scenario 1 --wandb rlcc-new_simulator --wandb_run_name tele_12_h_tgt_0.8_f_2.44_max_1_qnorm_80_p_1_rl_5_act_0.2_alc_1_sl_1_lr_0.01_envs_1_qp_4k_bst_lstm_lrelu0.01_wmpup_50_wmplen_1024_mbatch_8192_msz_4000_g_0.99_notanh --learning_rate 0.01 --agent ADPG --config telemetry_adpg --architecture 12 --max_num_updates 1000 --action_multiplier_inc 0.2 --action_multiplier_dec 0.2 --agent_features action qlength_reward --target 0.8 --factor 1.22 --power 1 --max_factor 1 --qnorm 80  --rollout_length 5 --action_loss_coeff 1 --loss_scale 1.0 --scenarios 2_1_qp_t 4_1_qp_t 8_1_qp_t 4_16_a2a_qp_t 8_8_a2a_qp_t 8_16_a2a_qp_t 32_1_qp_t  --limit_flows --use_lstm True --history_length 1 --leaky_relu_coeff 0.01 --warmup_updates 50 --warmup_length 1024 --max_batch_size 8192 --discount 0.99 --max_step_size 4000 --port_increment 50
# still doesn't really work
python3 run.py --envs_per_scenario 1 --wandb rlcc-new_simulator --wandb_run_name tele_12_h_tgt_0.8_f_2.44_max_10_qnorm_80_p_1_rl_5_act_0.2_alc_1_sl_1_lr_0.01_envs_1_qp_4k_bst_lstm_lrelu0.01_wmpup_50_wmplen_1024_mbatch_8192_msz_4000_g_0.99_notanh --learning_rate 0.01 --agent ADPG --config telemetry_adpg --architecture 12 --max_num_updates 1000 --action_multiplier_inc 0.2 --action_multiplier_dec 0.2 --agent_features action qlength_reward --target 0.8 --factor 1.22 --power 1 --max_factor 10 --qnorm 80  --rollout_length 5 --action_loss_coeff 1 --loss_scale 1.0 --scenarios 2_1_qp_t 4_1_qp_t 8_1_qp_t 4_16_a2a_qp_t 8_8_a2a_qp_t 8_16_a2a_qp_t 32_1_qp_t  --limit_flows --use_lstm True --history_length 1 --leaky_relu_coeff 0.01 --warmup_updates 50 --warmup_length 1024 --max_batch_size 8192 --discount 0.99 --max_step_size 4000 --port_increment 50

python3 run.py --envs_per_scenario 1 --wandb rlcc-new_simulator --wandb_run_name tele_12_h_tgt_0.8_f_0.0305_max_200_qnorm_1_p_1_rl_5_act_0.2_alc_1_sl_1_lr_0.01_envs_1_qp_4k_bst_lstm_lrelu0.01_wmpup_50_wmplen_1024_mbatch_8192_msz_4000_g_0.99_notanh --learning_rate 0.01 --agent ADPG --config telemetry_adpg --architecture 12 --max_num_updates 1000 --action_multiplier_inc 0.2 --action_multiplier_dec 0.2 --agent_features action qlength_reward --target 0.8 --factor 0.0305 --power 1 --max_factor 200 --qnorm 1  --rollout_length 5 --action_loss_coeff 1 --loss_scale 1.0 --scenarios 2_1_qp_t 4_1_qp_t 8_1_qp_t 4_16_a2a_qp_t 8_8_a2a_qp_t 8_16_a2a_qp_t 32_1_qp_t  --limit_flows --use_lstm True --history_length 1 --leaky_relu_coeff 0.01 --warmup_updates 50 --warmup_length 1024 --max_batch_size 8192 --discount 0.99 --max_step_size 4000 --port_increment 50
# it seems queue length value is too low
python3 run.py --envs_per_scenario 1 --wandb rlcc-new_simulator --wandb_run_name tele_12_h_tgt_0.8_f_0.04_max_200_qnorm_1_p_1_rl_5_act_0.2_alc_1_sl_1_lr_0.01_envs_1_qp_4k_bst_lstm_lrelu0.01_wmpup_50_wmplen_1024_mbatch_8192_msz_4000_g_0.99_notanh --learning_rate 0.01 --agent ADPG --config telemetry_adpg --architecture 12 --max_num_updates 1000 --action_multiplier_inc 0.2 --action_multiplier_dec 0.2 --agent_features action qlength_reward --target 0.8 --factor 0.04 --power 1 --max_factor 200 --qnorm 1  --rollout_length 5 --action_loss_coeff 1 --loss_scale 1.0 --scenarios 2_1_qp_t 4_1_qp_t 8_1_qp_t 4_16_a2a_qp_t 8_8_a2a_qp_t 8_16_a2a_qp_t 32_1_qp_t  --limit_flows --use_lstm True --history_length 1 --leaky_relu_coeff 0.01 --warmup_updates 50 --warmup_length 1024 --max_batch_size 8192 --discount 0.99 --max_step_size 4000 --port_increment 50
# still 2 _ 1 too slow
python3 run.py --envs_per_scenario 1 --wandb rlcc-new_simulator --wandb_run_name tele_12_h_tgt_0.8_f_0.04_max_150_qnorm_1_p_1_rl_5_act_0.2_alc_1_sl_1_lr_0.01_envs_1_qp_4k_bst_lstm_lrelu0.01_wmpup_50_wmplen_1024_mbatch_8192_msz_4000_g_0.99_notanh --learning_rate 0.01 --agent ADPG --config telemetry_adpg --architecture 12 --max_num_updates 1000 --action_multiplier_inc 0.2 --action_multiplier_dec 0.2 --agent_features action qlength_reward --target 0.8 --factor 0.04 --power 1 --max_factor 150 --qnorm 1  --rollout_length 5 --action_loss_coeff 1 --loss_scale 1.0 --scenarios 2_1_qp_t 4_1_qp_t 8_1_qp_t 4_16_a2a_qp_t 8_8_a2a_qp_t 8_16_a2a_qp_t 32_1_qp_t  --limit_flows --use_lstm True --history_length 1 --leaky_relu_coeff 0.01 --warmup_updates 50 --warmup_length 1024 --max_batch_size 8192 --discount 0.99 --max_step_size 4000 --port_increment 50
python3 run.py --envs_per_scenario 1 --wandb rlcc-new_simulator --wandb_run_name tele_12_h_tgt_0.8_f_0.03_max_150_qnorm_1_p_1_rl_5_act_0.2_alc_1_sl_1_lr_0.01_envs_1_qp_4k_bst_lstm_lrelu0.01_wmpup_50_wmplen_1024_mbatch_8192_msz_4000_g_0.99_notanh --learning_rate 0.01 --agent ADPG --config telemetry_adpg --architecture 12 --max_num_updates 1000 --action_multiplier_inc 0.2 --action_multiplier_dec 0.2 --agent_features action qlength_reward --target 0.8 --factor 0.03 --power 1 --max_factor 150 --qnorm 1  --rollout_length 5 --action_loss_coeff 1 --loss_scale 1.0 --scenarios 2_1_qp_t 4_1_qp_t 8_1_qp_t 4_16_a2a_qp_t 8_8_a2a_qp_t 8_16_a2a_qp_t 32_1_qp_t  --limit_flows --use_lstm True --history_length 1 --leaky_relu_coeff 0.01 --warmup_updates 50 --warmup_length 1024 --max_batch_size 8192 --discount 0.99 --max_step_size 4000 --port_increment 50
# 0.04 results in smaller qlen than 0,03 -> it seems that 0.03 isn't good
python3 run.py --envs_per_scenario 1 --wandb rlcc-new_simulator --wandb_run_name tele_12_h_tgt_0.8_f_0.045_max_150_qnorm_1_p_1_rl_5_act_0.2_alc_1_sl_1_lr_0.01_envs_1_qp_4k_bst_lstm_lrelu0.01_wmpup_50_wmplen_1024_mbatch_8192_msz_4000_g_0.99_notanh --learning_rate 0.01 --agent ADPG --config telemetry_adpg --architecture 12 --max_num_updates 1000 --action_multiplier_inc 0.2 --action_multiplier_dec 0.2 --agent_features action qlength_reward --target 0.8 --factor 0.045 --power 1 --max_factor 150 --qnorm 1  --rollout_length 5 --action_loss_coeff 1 --loss_scale 1.0 --scenarios 2_1_qp_t 4_1_qp_t 8_1_qp_t 4_16_a2a_qp_t 8_8_a2a_qp_t 8_16_a2a_qp_t 32_1_qp_t  --limit_flows --use_lstm True --history_length 1 --leaky_relu_coeff 0.01 --warmup_updates 50 --warmup_length 1024 --max_batch_size 8192 --discount 0.99 --max_step_size 4000 --port_increment 50
python3 run.py --envs_per_scenario 1 --wandb rlcc-new_simulator --wandb_run_name tele_12_h_tgt_0.8_f_0.05_max_150_qnorm_1_p_1_rl_5_act_0.2_alc_1_sl_1_lr_0.01_envs_1_qp_4k_bst_lstm_lrelu0.01_wmpup_50_wmplen_1024_mbatch_8192_msz_4000_g_0.99_notanh --learning_rate 0.01 --agent ADPG --config telemetry_adpg --architecture 12 --max_num_updates 1000 --action_multiplier_inc 0.2 --action_multiplier_dec 0.2 --agent_features action qlength_reward --target 0.8 --factor 0.05 --power 1 --max_factor 150 --qnorm 1  --rollout_length 5 --action_loss_coeff 1 --loss_scale 1.0 --scenarios 2_1_qp_t 4_1_qp_t 8_1_qp_t 4_16_a2a_qp_t 8_8_a2a_qp_t 8_16_a2a_qp_t 32_1_qp_t  --limit_flows --use_lstm True --history_length 1 --leaky_relu_coeff 0.01 --warmup_updates 50 --warmup_length 1024 --max_batch_size 8192 --discount 0.99 --max_step_size 4000 --port_increment 50

# we tried also 0.06 but didnt work that great
# trying either to lower max to 100 or to add update_delay

python3 run.py --envs_per_scenario 1 --wandb rlcc-new_simulator --wandb_run_name tele_12_h_tgt_0.8_f_0.06_max_150_qnorm_1_p_1_rl_5_act_0.2_alc_1_sl_1_lr_0.01_envs_1_qp_4k_bst_lstm_lrelu0.01_wmpup_50_wmplen_1024_mbatch_8192_msz_4000_g_0.99_notanh_updelay --learning_rate 0.01 --agent ADPG --config telemetry_adpg --architecture 12 --max_num_updates 1000 --action_multiplier_inc 0.2 --action_multiplier_dec 0.2 --agent_features action qlength_reward update_delay --target 0.8 --factor 0.06 --power 1 --max_factor 150 --qnorm 1  --rollout_length 5 --action_loss_coeff 1 --loss_scale 1.0 --scenarios 2_1_qp_t 4_1_qp_t 8_1_qp_t 4_16_a2a_qp_t 8_8_a2a_qp_t 8_16_a2a_qp_t 32_1_qp_t  --limit_flows --use_lstm True --history_length 1 --leaky_relu_coeff 0.01 --warmup_updates 50 --warmup_length 1024 --max_batch_size 8192 --discount 0.99 --max_step_size 4000 --port_increment 50
python3 run.py --envs_per_scenario 1 --wandb rlcc-new_simulator --wandb_run_name tele_12_h_tgt_0.8_f_0.06_max_100_qnorm_1_p_1_rl_5_act_0.2_alc_1_sl_1_lr_0.01_envs_1_qp_4k_bst_lstm_lrelu0.01_wmpup_50_wmplen_1024_mbatch_8192_msz_4000_g_0.99_notanh --learning_rate 0.01 --agent ADPG --config telemetry_adpg --architecture 12 --max_num_updates 1000 --action_multiplier_inc 0.2 --action_multiplier_dec 0.2 --agent_features action qlength_reward --target 0.8 --factor 0.06 --power 1 --max_factor 100 --qnorm 1  --rollout_length 5 --action_loss_coeff 1 --loss_scale 1.0 --scenarios 2_1_qp_t 4_1_qp_t 8_1_qp_t 4_16_a2a_qp_t 8_8_a2a_qp_t 8_16_a2a_qp_t 32_1_qp_t  --limit_flows --use_lstm True --history_length 1 --leaky_relu_coeff 0.01 --warmup_updates 50 --warmup_length 1024 --max_batch_size 8192 --discount 0.99 --max_step_size 4000 --port_increment 50

# no factor anymore - using graphs
python3 run.py --envs_per_scenario 1 --wandb rlcc-new_simulator --wandb_run_name tele_12_h_tgt_5_f_max_10_qnorm_40_p_1_rl_5_act_0.2_alc_1_sl_1_lr_0.01_envs_1_qp_4k_bst_lstm_lrelu0.01_wmpup_50_wmplen_1024_mbatch_8192_msz_4000_g_0.99_notanh --learning_rate 0.01 --agent ADPG --config telemetry_adpg --architecture 12 --max_num_updates 1000 --action_multiplier_inc 0.2 --action_multiplier_dec 0.2 --agent_features action qlength_reward --target 5 --power 1 --max_factor 10 --qnorm 40  --rollout_length 5 --action_loss_coeff 1 --loss_scale 1.0 --scenarios 2_1_qp_t 4_1_qp_t 8_1_qp_t 4_16_a2a_qp_t 8_8_a2a_qp_t 8_16_a2a_qp_t 32_1_qp_t  --limit_flows --use_lstm True --history_length 1 --leaky_relu_coeff 0.01 --warmup_updates 50 --warmup_length 1024 --max_batch_size 8192 --discount 0.99 --max_step_size 4000 --port_increment 50
python3 run.py --envs_per_scenario 1 --wandb rlcc-new_simulator --wandb_run_name tele_12_h_tgt_5_f_max_10_qnorm_40_p_1_rl_5_act_0.2_alc_1_sl_1_lr_0.01_envs_1_qp_4k_bst_lstm_lrelu0.01_wmpup_50_wmplen_1024_mbatch_8192_msz_4000_g_0.99_notanh_updaly --learning_rate 0.01 --agent ADPG --config telemetry_adpg --architecture 12 --max_num_updates 1000 --action_multiplier_inc 0.2 --action_multiplier_dec 0.2 --agent_features action qlength_reward update_delay --target 5 --power 1 --max_factor 10 --qnorm 40  --rollout_length 5 --action_loss_coeff 1 --loss_scale 1.0 --scenarios 2_1_qp_t 4_1_qp_t 8_1_qp_t 4_16_a2a_qp_t 8_8_a2a_qp_t 8_16_a2a_qp_t 32_1_qp_t  --limit_flows --use_lstm True --history_length 1 --leaky_relu_coeff 0.01 --warmup_updates 50 --warmup_length 1024 --max_batch_size 8192 --discount 0.99 --max_step_size 4000 --port_increment 50

# starting with new wandb rep
python3 run.py --envs_per_scenario 1 --wandb rlcc-new_simulator_v2 --wandb_run_name tele_12_h_tgt_5_f_max_10_qnorm_40_p_1_rl_5_act_0.2_alc_1_sl_0.1_lr_0.01_envs_1_qp_4k_bst_lstm_lrelu0.01_wmpup_50_wmplen_1024_mbatch_8192_msz_4000_g_0.99_notanh --learning_rate 0.01 --agent ADPG --config telemetry_adpg --architecture 12 --max_num_updates 1000 --action_multiplier_inc 0.2 --action_multiplier_dec 0.2 --agent_features action qlength_reward --target 5 --power 1 --max_factor 10 --qnorm 40  --rollout_length 5 --action_loss_coeff 1 --loss_scale 0.1 --scenarios 2_1_qp_t 4_1_qp_t 8_1_qp_t 4_16_a2a_qp_t 8_8_a2a_qp_t 8_16_a2a_qp_t 32_1_qp_t  --limit_flows --use_lstm True --history_length 1 --leaky_relu_coeff 0.01 --warmup_updates 50 --warmup_length 1024 --max_batch_size 8192 --discount 0.99 --max_step_size 4000 --port_increment 50
python3 run.py --envs_per_scenario 1 --wandb rlcc-new_simulator_v2 --wandb_run_name tele_12_h_tgt_5_f_max_10_qnorm_40_p_1_rl_5_act_0.2_alc_1_sl_0.1_lr_0.01_envs_1_qp_4k_bst_lstm_lrelu0.01_wmpup_50_wmplen_1024_mbatch_8192_msz_4000_g_0.99_notanh_updaly --learning_rate 0.01 --agent ADPG --config telemetry_adpg --architecture 12 --max_num_updates 1000 --action_multiplier_inc 0.2 --action_multiplier_dec 0.2 --agent_features action qlength_reward update_delay --target 5 --power 1 --max_factor 10 --qnorm 40  --rollout_length 5 --action_loss_coeff 1 --loss_scale 0.1 --scenarios 2_1_qp_t 4_1_qp_t 8_1_qp_t 4_16_a2a_qp_t 8_8_a2a_qp_t 8_16_a2a_qp_t 32_1_qp_t  --limit_flows --use_lstm True --history_length 1 --leaky_relu_coeff 0.01 --warmup_updates 50 --warmup_length 1024 --max_batch_size 8192 --discount 0.99 --max_step_size 4000 --port_increment 50

# trying to re-produce qp results
python3 run.py --envs_per_scenario 1 --wandb rlcc-new_simulator_v2 --wandb_run_name rtt_12_h_tgt_0.52_max_1.5_p_1_rl_3_act_0.2_alc_1_sl_2.5_lr_0.01_envs_1_qp_4k_bst_hl_2_relu_wmpup_50_wmplen_1024_mbatch_8192_msz_4000 --learning_rate 0.01 --agent ADPG --config rtt_adpg --architecture 12 --max_num_updates 1000 --action_multiplier_inc 0.2 --action_multiplier_dec 0.2 --agent_features action rtt_reward --target 0.52 --max_factor 1.5 --base_rtt 8192 --rollout_length 3 --action_loss_coeff 1 --loss_scale 2.5 --scenarios 2_1_qp 4_1_qp 8_1_qp 4_16_a2a_qp 8_8_a2a_qp 8_16_a2a_qp 32_1_qp --limit_flows --use_lstm False --history_length 2 --leaky_relu_coeff 0.0 --warmup_updates 50 --warmup_length 1024  --max_batch_size 8192 --discount 0.99 --max_step_size 4000 --port_increment 50

python3 run.py --envs_per_scenario 1 --wandb rlcc-new_simulator_v2 --wandb_run_name rtt_12_h_tgt_0.064_max_1.5_p_1_rl_3_act_0.2_alc_1_sl_2.5_lr_0.01_envs_1_qp_4k_bst_hl_2_relu_wmpup_50_wmplen_1024_mbatch_8192_msz_4000 --learning_rate 0.01 --agent ADPG --config rtt_adpg --architecture 12 --max_num_updates 1000 --action_multiplier_inc 0.2 --action_multiplier_dec 0.2 --agent_features action rtt_reward --target 0.064 --max_factor 1.5 --base_rtt 8192 --rollout_length 3 --action_loss_coeff 1 --loss_scale 2.5 --scenarios 2_1_qp 4_1_qp 8_1_qp 4_16_a2a_qp 8_8_a2a_qp 8_16_a2a_qp 32_1_qp --limit_flows --use_lstm False --history_length 2 --leaky_relu_coeff 0.0 --warmup_updates 50 --warmup_length 1024  --max_batch_size 8192 --discount 0.99 --max_step_size 4000 --port_increment 50
python3 run.py --envs_per_scenario 1 --wandb rlcc-new_simulator_v2 --wandb_run_name rtt_12_h_tgt_0.064_max_1.5_f_12.5_rl_3_act_0.2_alc_1_sl_2.5_lr_0.01_envs_1_qp_4k_bst_hl_2_relu_wmpup_50_wmplen_1024_mbatch_8192_msz_4000 --learning_rate 0.01 --agent ADPG --config rtt_adpg --architecture 12 --max_num_updates 1000 --action_multiplier_inc 0.2 --action_multiplier_dec 0.2 --agent_features action rtt_reward --target 0.064 --factor 12.5 --max_factor 1.5 --base_rtt 8192 --rollout_length 3 --action_loss_coeff 1 --loss_scale 1 --scenarios 2_1_qp 4_1_qp 8_1_qp 4_16_a2a_qp 8_8_a2a_qp 8_16_a2a_qp 32_1_qp --limit_flows --use_lstm False --history_length 2 --leaky_relu_coeff 0.0 --warmup_updates 50 --warmup_length 1024  --max_batch_size 8192 --discount 0.99 --max_step_size 4000 --port_increment 50
python3 run.py --envs_per_scenario 1 --wandb rlcc-new_simulator_v2 --wandb_run_name rtt_12_h_tgt_0.064_max_1.5_f_10_rl_3_act_0.2_alc_1_sl_2.5_lr_0.01_envs_1_qp_4k_bst_hl_2_relu_wmpup_50_wmplen_1024_mbatch_8192_msz_4000 --learning_rate 0.01 --agent ADPG --config rtt_adpg --architecture 12 --max_num_updates 1000 --action_multiplier_inc 0.2 --action_multiplier_dec 0.2 --agent_features action rtt_reward --target 0.064 --factor 10 --max_factor 1.5 --base_rtt 8192 --rollout_length 3 --action_loss_coeff 1 --loss_scale 1 --scenarios 2_1_qp 4_1_qp 8_1_qp 4_16_a2a_qp 8_8_a2a_qp 8_16_a2a_qp 32_1_qp --limit_flows --use_lstm False --history_length 2 --leaky_relu_coeff 0.0 --warmup_updates 50 --warmup_length 1024  --max_batch_size 8192 --discount 0.99 --max_step_size 4000 --port_increment 50
python3 run.py --envs_per_scenario 1 --wandb rlcc-new_simulator_v2 --wandb_run_name rtt_12_h_tgt_0.064_max_1.5_f_10_rl_3_act_0.2_alc_1_sl_2.5_lr_0.01_envs_1_qp_4k_bst_hl_2_relu_wmpup_50_wmplen_1024_mbatch_8192_msz_4000_updatedly --learning_rate 0.01 --agent ADPG --config rtt_adpg --architecture 12 --max_num_updates 1000 --action_multiplier_inc 0.2 --action_multiplier_dec 0.2 --agent_features action rtt_reward update_delay --target 0.064 --factor 10 --max_factor 1.5 --base_rtt 8192 --rollout_length 3 --action_loss_coeff 1 --loss_scale 1 --scenarios 2_1_qp 4_1_qp 8_1_qp 4_16_a2a_qp 8_8_a2a_qp 8_16_a2a_qp 32_1_qp --limit_flows --use_lstm False --history_length 2 --leaky_relu_coeff 0.0 --warmup_updates 50 --warmup_length 1024  --max_batch_size 8192 --discount 0.99 --max_step_size 4000 --port_increment 50

python3 run.py --envs_per_scenario 1 --wandb rlcc-new_simulator_v2 --wandb_run_name tele_12_h_tgt_0.064_max_1.5_f_10_qnorm_1_rl_5_act_0.2_alc_1_sl_0.1_lr_0.01_envs_1_qp_4k_bst_lstm_lrelu0.01_wmpup_50_wmplen_1024_mbatch_8192_msz_4000_g_0.99_notanh_updaly --learning_rate 0.01 --agent ADPG --config telemetry_adpg --architecture 12 --max_num_updates 1000 --action_multiplier_inc 0.2 --action_multiplier_dec 0.2 --agent_features action qlength_reward --target 0.064 --factor 10 --max_factor 1.5 --qnorm 40  --rollout_length 5 --action_loss_coeff 1 --loss_scale 0.1 --scenarios 2_1_qp_t 4_1_qp_t 8_1_qp_t 4_16_a2a_qp_t 8_8_a2a_qp_t 8_16_a2a_qp_t 32_1_qp_t  --limit_flows --use_lstm True --history_length 1 --leaky_relu_coeff 0.01 --warmup_updates 50 --warmup_length 1024 --max_batch_size 4000 --discount 0.99 --max_step_size 4000 --port_increment 50
python3 run.py --envs_per_scenario 1 --wandb rlcc-new_simulator_v2 --wandb_run_name tele_12_h_tgt_0.064_max_1.5_f_10_qnorm_1_rl_5_act_0.2_alc_1_sl_1_lr_0.01_envs_1_qp_4k_bst_lstm_lrelu0.01_wmpup_50_wmplen_1024_mbatch_8192_msz_4000_g_0.99_notanh_updaly --learning_rate 0.01 --agent ADPG --config telemetry_adpg --architecture 12 --max_num_updates 1000 --action_multiplier_inc 0.2 --action_multiplier_dec 0.2 --agent_features action qlength_reward --target 0.064 --factor 10 --max_factor 1.5 --qnorm 40  --rollout_length 5 --action_loss_coeff 1 --loss_scale 1 --scenarios 2_1_qp_t 4_1_qp_t 8_1_qp_t 4_16_a2a_qp_t 8_8_a2a_qp_t 8_16_a2a_qp_t 32_1_qp_t  --limit_flows --use_lstm True --history_length 1 --leaky_relu_coeff 0.01 --warmup_updates 50 --warmup_length 1024 --max_batch_size 4000 --discount 0.99 --max_step_size 4000 --port_increment 50


python3 run.py --envs_per_scenario 1 --wandb rlcc-new_simulator_v2 --wandb_run_name rtt_12_h_tgt_0.064_max_1.5_f_10_rl_3_act_0.2_alc_1_sl_5_lr_0.01_envs_1_qp_4k_bst_hl_2_relu_wmpup_50_wmplen_1024_mbatch_8192_msz_5000_updatedly --learning_rate 0.01 --agent ADPG --config rtt_adpg --architecture 12 --max_num_updates 1000 --action_multiplier_inc 0.2 --action_multiplier_dec 0.2 --agent_features action rtt_reward update_delay --target 0.064 --factor 10 --max_factor 1.5 --base_rtt 8192 --rollout_length 3 --action_loss_coeff 1 --loss_scale 5 --scenarios 2_1_qp 4_1_qp 8_1_qp 4_16_a2a_qp 8_8_a2a_qp 8_16_a2a_qp 32_1_qp --limit_flows --use_lstm False --history_length 2 --leaky_relu_coeff 0.0 --warmup_updates 50 --warmup_length 1024  --max_batch_size 8192 --discount 0.99 --max_step_size 5000 --port_increment 50

# still telemetery makes problems bringing back tanh and removing leaky
python3 run.py --envs_per_scenario 1 --wandb rlcc-new_simulator_v2 --wandb_run_name tele_12_h_tgt_0.064_max_1.5_f_10_qnorm_1_rl_5_act_0.2_alc_1_sl_1_lr_0.01_envs_1_qp_4k_bst_lstm_relu_wmpup_50_wmplen_1024_mbatch_8192_msz_4000_g_0.99 --learning_rate 0.01 --agent ADPG --config telemetry_adpg --architecture 12 --max_num_updates 1000 --action_multiplier_inc 0.2 --action_multiplier_dec 0.2 --agent_features action qlength_reward --target 0.064 --factor 10 --max_factor 1.5 --qnorm 40  --rollout_length 5 --action_loss_coeff 1 --loss_scale 1 --scenarios 2_1_qp_t 4_1_qp_t 8_1_qp_t 4_16_a2a_qp_t 8_8_a2a_qp_t 8_16_a2a_qp_t 32_1_qp_t  --limit_flows --use_lstm True --history_length 1 --leaky_relu_coeff 0.0 --warmup_updates 50 --warmup_length 1024 --max_batch_size 4000 --discount 0.99 --max_step_size 4000 --port_increment 50

# rtt but with factor only when we need to increase
python3 run.py --envs_per_scenario 1 --wandb rlcc-new_simulator_v2 --wandb_run_name rtt_12_h_tgt_0.064_max_1.5_f_10_rl_3_act_0.2_alc_1_sl_5_lr_0.01_envs_1_qp_4k_bst_hl_2_relu_wmpup_50_wmplen_1024_mbatch_8192_msz_5000_f_onlywhenincrease --learning_rate 0.01 --agent ADPG --config rtt_adpg --architecture 12 --max_num_updates 1000 --action_multiplier_inc 0.2 --action_multiplier_dec 0.2 --agent_features action rtt_reward --target 0.064 --factor 10 --max_factor 1.5 --base_rtt 8192 --rollout_length 3 --action_loss_coeff 1 --loss_scale 5 --scenarios 2_1_qp 4_1_qp 8_1_qp 4_16_a2a_qp 8_8_a2a_qp 8_16_a2a_qp 32_1_qp --limit_flows --use_lstm False --history_length 2 --leaky_relu_coeff 0.0 --warmup_updates 50 --warmup_length 1024  --max_batch_size 8192 --discount 0.99 --max_step_size 5000 --port_increment 50

# using 1-max_rtt and different reward scaling
python3 run.py --envs_per_scenario 1 --wandb rlcc-new_simulator_v2 --wandb_run_name rtt_12_h_tgt_0.064_max_1.5_f_10_rl_3_act_0.2_alc_1_sl_5_lr_0.01_envs_1_qp_4k_bst_hl_2_relu_wmpup_50_wmplen_1024_mbatch_8192_msz_5000_2scaling --learning_rate 0.01 --agent ADPG --config rtt_adpg --architecture 12 --max_num_updates 1000 --action_multiplier_inc 0.2 --action_multiplier_dec 0.2 --agent_features action rtt_reward --target 0.064 --factor 10 --max_factor 1.5 --base_rtt 8192 --rollout_length 3 --action_loss_coeff 1 --loss_scale 5 --scenarios 2_1_qp 4_1_qp 8_1_qp 4_16_a2a_qp 8_8_a2a_qp 8_16_a2a_qp 32_1_qp --limit_flows --use_lstm False --history_length 2 --leaky_relu_coeff 0.0 --warmup_updates 50 --warmup_length 1024  --max_batch_size 8192 --discount 0.99 --max_step_size 5000 --norm_reward --port_increment 50
# no re-reward scaling but with no max
python3 run.py --envs_per_scenario 1 --wandb rlcc-new_simulator_v2 --wandb_run_name rtt_12_h_tgt_0.064_max_1.5_f_12_rl_3_act_0.2_alc_1_sl_5_lr_0.01_envs_1_qp_4k_bst_hl_2_relu_wmpup_50_wmplen_1024_mbatch_8192_msz_5000_no_max --learning_rate 0.01 --agent ADPG --config rtt_adpg --architecture 12 --max_num_updates 1000 --action_multiplier_inc 0.2 --action_multiplier_dec 0.2 --agent_features action rtt_reward --target 0.064 --factor 12 --max_factor 1.5 --base_rtt 8192 --rollout_length 3 --action_loss_coeff 1 --loss_scale 5 --scenarios 2_1_qp 4_1_qp 8_1_qp 4_16_a2a_qp 8_8_a2a_qp 8_16_a2a_qp 32_1_qp --limit_flows --use_lstm False --history_length 2 --leaky_relu_coeff 0.0 --warmup_updates 50 --warmup_length 1024  --max_batch_size 8192 --discount 0.99 --max_step_size 5000 --port_increment 50

# with max but with no 2_to_1
python3 run.py --envs_per_scenario 1 --wandb rlcc-new_simulator_v2 --wandb_run_name rtt_12_h_tgt_0.064_max_1.5_f_12_rl_3_act_0.2_alc_1_sl_5_lr_0.01_envs_1_qp_4k_bst_hl_2_relu_wmpup_50_wmplen_1024_mbatch_8192_msz_5000_max_no_2_1 --learning_rate 0.01 --agent ADPG --config rtt_adpg --architecture 12 --max_num_updates 1000 --action_multiplier_inc 0.2 --action_multiplier_dec 0.2 --agent_features action rtt_reward --target 0.064 --factor 12 --max_factor 1.5 --base_rtt 8192 --rollout_length 3 --action_loss_coeff 1 --loss_scale 5 --scenarios 4_1_qp 8_1_qp 4_16_a2a_qp 8_8_a2a_qp 8_16_a2a_qp 32_1_qp --limit_flows --use_lstm False --history_length 2 --leaky_relu_coeff 0.0 --warmup_updates 50 --warmup_length 1024  --max_batch_size 8192 --discount 0.99 --max_step_size 5000 --port_increment 50
# with max and 2_to_1 and everything
python3 run.py --envs_per_scenario 1 --wandb rlcc-new_simulator_v2 --wandb_run_name rtt_12_h_tgt_0.064_max_1.5_f_12.5_rl_3_act_0.2_alc_1_sl_5_lr_0.01_envs_1_qp_4k_bst_hl_2_relu_wmpup_50_wmplen_1024_mbatch_8192_msz_5000_max --learning_rate 0.01 --agent ADPG --config rtt_adpg --architecture 12 --max_num_updates 1000 --action_multiplier_inc 0.2 --action_multiplier_dec 0.2 --agent_features action rtt_reward --target 0.064 --factor 12.5 --max_factor 1.5 --base_rtt 8192 --rollout_length 3 --action_loss_coeff 1 --loss_scale 5 --scenarios 2_1_qp 4_1_qp 8_1_qp 4_16_a2a_qp 8_8_a2a_qp 8_16_a2a_qp 32_1_qp --limit_flows --use_lstm False --history_length 2 --leaky_relu_coeff 0.0 --warmup_updates 50 --warmup_length 1024  --max_batch_size 8192 --discount 0.99 --max_step_size 5000 --port_increment 50
# no max and no 2_to_1
python3 run.py --envs_per_scenario 1 --wandb rlcc-new_simulator_v2 --wandb_run_name rtt_12_h_tgt_0.064_max_1.5_f_12_rl_3_act_0.2_alc_1_sl_5_lr_0.01_envs_1_qp_4k_bst_hl_2_relu_wmpup_50_wmplen_1024_mbatch_8192_msz_5000_no_max_no_2_1 --learning_rate 0.01 --agent ADPG --config rtt_adpg --architecture 12 --max_num_updates 1000 --action_multiplier_inc 0.2 --action_multiplier_dec 0.2 --agent_features action rtt_reward --target 0.064 --factor 12 --max_factor 1.5 --base_rtt 8192 --rollout_length 3 --action_loss_coeff 1 --loss_scale 5 --scenarios 4_1_qp 8_1_qp 4_16_a2a_qp 8_8_a2a_qp 8_16_a2a_qp 32_1_qp --limit_flows --use_lstm False --history_length 2 --leaky_relu_coeff 0.0 --warmup_updates 50 --warmup_length 1024  --max_batch_size 8192 --discount 0.99 --max_step_size 5000 --port_increment 50

"""
Starting to run telemetry as percentage of buffersize
This means that Q-length 
"""
python3 run.py --envs_per_scenario 1 --wandb rlcc-new_simulator_v2 --wandb_run_name tele_12_h_tgt_0.005_max_0.05_f_10_rl_3_act_0.2_alc_1_sl_5_lr_0.01_envs_1_qp_4k_bst_hl_2_relu_wmpup_50_wmplen_1024_mbatch_8192_msz_5000_max_no_2_1 --learning_rate 0.01 --qnorm 1 --agent ADPG --config telemetry_adpg --architecture 12 --max_num_updates 1000 --action_multiplier_inc 0.2 --action_multiplier_dec 0.2 --agent_features action qlength_reward --target 0.005 --factor 10 --max_factor 0.05 --base_rtt 8192 --rollout_length 3 --action_loss_coeff 1 --loss_scale 5 --scenarios 4_1_qp_t 8_1_qp_t 4_16_a2a_qp_t 8_8_a2a_qp_t 8_16_a2a_qp_t 32_1_qp_t --limit_flows --use_lstm False --history_length 2 --leaky_relu_coeff 0.0 --warmup_updates 50 --warmup_length 1024  --max_batch_size 8192 --discount 0.99 --max_step_size 5000 --port_increment 50
python3 run.py --envs_per_scenario 1 --wandb rlcc-new_simulator_v2 --wandb_run_name tele_12_h_tgt_0.005_max_0.05_f_100_rl_3_act_0.2_alc_1_sl_5_lr_0.01_envs_1_qp_4k_bst_hl_2_relu_wmpup_50_wmplen_1024_mbatch_8192_msz_5000_max_no_2_1 --learning_rate 0.01 --qnorm 1 --agent ADPG --config telemetry_adpg --architecture 12 --max_num_updates 1000 --action_multiplier_inc 0.2 --action_multiplier_dec 0.2 --agent_features action qlength_reward --target 0.005 --factor 100 --max_factor 0.05 --base_rtt 8192 --rollout_length 3 --action_loss_coeff 1 --loss_scale 5 --scenarios 4_1_qp_t 8_1_qp_t 4_16_a2a_qp_t 8_8_a2a_qp_t 8_16_a2a_qp_t 32_1_qp_t --limit_flows --use_lstm False --history_length 2 --leaky_relu_coeff 0.0 --warmup_updates 50 --warmup_length 1024  --max_batch_size 8192 --discount 0.99 --max_step_size 5000 --port_increment 50


# comparing LSTM with different architectures
python3 run.py --envs_per_scenario 1 --wandb rlcc-new_simulator_v2 --wandb_run_name rtt_12_h_tgt_0.064_max_1.5_f_12_rl_3_act_0.2_alc_1_sl_5_lr_0.01_envs_1_qp_4k_bst_lstm_relu_wmpup_50_wmplen_1024_mbatch_8192_msz_5000_max_no_2_1 --learning_rate 0.01 --agent ADPG --config rtt_adpg --architecture 12 --max_num_updates 1000 --action_multiplier_inc 0.2 --action_multiplier_dec 0.2 --agent_features action rtt_reward --target 0.064 --factor 12 --max_factor 1.5 --base_rtt 8192 --rollout_length 3 --action_loss_coeff 1 --loss_scale 5 --scenarios 4_1_qp 8_1_qp 4_16_a2a_qp 8_8_a2a_qp 8_16_a2a_qp 32_1_qp --limit_flows --use_lstm True --history_length 1 --leaky_relu_coeff 0.0 --warmup_updates 50 --warmup_length 1024  --max_batch_size 8192 --discount 0.99 --max_step_size 5000 --port_increment 50
python3 run.py --envs_per_scenario 1 --wandb rlcc-new_simulator_v2 --wandb_run_name rtt_12_12_h_tgt_0.064_max_1.5_f_12_rl_3_act_0.2_alc_1_sl_5_lr_0.01_envs_1_qp_4k_bst_hl_2_relu_wmpup_50_wmplen_1024_mbatch_8192_msz_5000_max_no_2_1 --learning_rate 0.01 --agent ADPG --config rtt_adpg --architecture 12 12 --max_num_updates 1000 --action_multiplier_inc 0.2 --action_multiplier_dec 0.2 --agent_features action rtt_reward --target 0.064 --factor 12 --max_factor 1.5 --base_rtt 8192 --rollout_length 3 --action_loss_coeff 1 --loss_scale 5 --scenarios 4_1_qp 8_1_qp 4_16_a2a_qp 8_8_a2a_qp 8_16_a2a_qp 32_1_qp --limit_flows --use_lstm False --history_length 2 --leaky_relu_coeff 0.0 --warmup_updates 50 --warmup_length 1024  --max_batch_size 8192 --discount 0.99 --max_step_size 5000 --port_increment 50
python3 run.py --envs_per_scenario 1 --wandb rlcc-new_simulator_v2 --wandb_run_name rtt_12_12_12_h_tgt_0.064_max_1.5_f_12_rl_3_act_0.2_alc_1_sl_5_lr_0.01_envs_1_qp_4k_bst_hl_2_relu_wmpup_50_wmplen_1024_mbatch_8192_msz_5000_max_no_2_1 --learning_rate 0.01 --agent ADPG --config rtt_adpg --architecture 12 12 12 --max_num_updates 1000 --action_multiplier_inc 0.2 --action_multiplier_dec 0.2 --agent_features action rtt_reward --target 0.064 --factor 12 --max_factor 1.5 --base_rtt 8192 --rollout_length 3 --action_loss_coeff 1 --loss_scale 5 --scenarios 4_1_qp 8_1_qp 4_16_a2a_qp 8_8_a2a_qp 8_16_a2a_qp 32_1_qp --limit_flows --use_lstm False --history_length 2 --leaky_relu_coeff 0.0 --warmup_updates 50 --warmup_length 1024  --max_batch_size 8192 --discount 0.99 --max_step_size 5000 --port_increment 50

# seems that max factor 5 % is too high for a2a so moving to 2% which is 20 Kb

python3 run.py --envs_per_scenario 1 --wandb rlcc-new_simulator_v2 --wandb_run_name tele_12_h_tgt_0.005_max_0.02_f_150_rl_3_act_0.2_alc_1_sl_5_lr_0.01_envs_1_qp_4k_bst_hl_2_relu_wmpup_50_wmplen_1024_mbatch_8192_msz_5000_max_no_2_1 --learning_rate 0.01 --qnorm 1 --agent ADPG --config telemetry_adpg --architecture 12 --max_num_updates 1000 --action_multiplier_inc 0.2 --action_multiplier_dec 0.2 --agent_features action qlength_reward --target 0.005 --factor 150 --max_factor 0.02 --base_rtt 8192 --rollout_length 3 --action_loss_coeff 1 --loss_scale 5 --scenarios 4_1_qp_t 8_1_qp_t 4_16_a2a_qp_t 8_8_a2a_qp_t 8_16_a2a_qp_t 32_1_qp_t --limit_flows --use_lstm False --history_length 2 --leaky_relu_coeff 0.0 --warmup_updates 50 --warmup_length 1024  --max_batch_size 8192 --discount 0.99 --max_step_size 5000 --port_increment 50
python3 run.py --envs_per_scenario 1 --wandb rlcc-new_simulator_v2 --wandb_run_name tele_12_h_tgt_0.007_max_0.02_f_150_rl_3_act_0.2_alc_1_sl_5_lr_0.01_envs_1_qp_4k_bst_hl_2_relu_wmpup_50_wmplen_1024_mbatch_8192_msz_5000_max_no_2_1 --learning_rate 0.01 --qnorm 1 --agent ADPG --config telemetry_adpg --architecture 12 --max_num_updates 1000 --action_multiplier_inc 0.2 --action_multiplier_dec 0.2 --agent_features action qlength_reward --target 0.007 --factor 150 --max_factor 0.02 --base_rtt 8192 --rollout_length 3 --action_loss_coeff 1 --loss_scale 5 --scenarios 4_1_qp_t 8_1_qp_t 4_16_a2a_qp_t 8_8_a2a_qp_t 8_16_a2a_qp_t 32_1_qp_t --limit_flows --use_lstm False --history_length 2 --leaky_relu_coeff 0.0 --warmup_updates 50 --warmup_length 1024  --max_batch_size 8192 --discount 0.99 --max_step_size 5000 --port_increment 50
python3 run.py --envs_per_scenario 1 --wandb rlcc-new_simulator_v2 --wandb_run_name tele_12_h_tgt_0.003_max_0.02_f_150_rl_3_act_0.2_alc_1_sl_5_lr_0.01_envs_1_qp_4k_bst_hl_2_relu_wmpup_50_wmplen_1024_mbatch_8192_msz_5000_max_no_2_1 --learning_rate 0.01 --qnorm 1 --agent ADPG --config telemetry_adpg --architecture 12 --max_num_updates 1000 --action_multiplier_inc 0.2 --action_multiplier_dec 0.2 --agent_features action qlength_reward --target 0.003 --factor 150 --max_factor 0.02 --base_rtt 8192 --rollout_length 3 --action_loss_coeff 1 --loss_scale 5 --scenarios 4_1_qp_t 8_1_qp_t 4_16_a2a_qp_t 8_8_a2a_qp_t 8_16_a2a_qp_t 32_1_qp_t --limit_flows --use_lstm False --history_length 2 --leaky_relu_coeff 0.0 --warmup_updates 50 --warmup_length 1024  --max_batch_size 8192 --discount 0.99 --max_step_size 5000 --port_increment 50

# fairness issues with 4 16 and 4 64
python3 run.py --envs_per_scenario 1 --wandb rlcc-new_simulator_v2 --wandb_run_name rtt_12_h_tgt_0.064_max_1.5_f_15_rl_3_act_0.2_alc_1_sl_2.5_lr_0.01_envs_1_qp_4k_bst_hl_2_relu_wmpup_50_wmplen_1024_mbatch_8192_msz_5000_max_no_2_1 --learning_rate 0.01 --agent ADPG --config rtt_adpg --architecture 12 --max_num_updates 1000 --action_multiplier_inc 0.2 --action_multiplier_dec 0.2 --agent_features action rtt_reward --target 0.064 --factor 15 --max_factor 1.5 --base_rtt 8192 --rollout_length 5 --action_loss_coeff 1 --loss_scale 2.5 --scenarios 4_1_qp 4_16_qp 8_1_qp 4_16_a2a_qp 8_8_a2a_qp 8_16_a2a_qp 32_1_qp --limit_flows --use_lstm False --history_length 2 --leaky_relu_coeff 0.0 --warmup_updates 50 --warmup_length 1024  --max_batch_size 8192 --discount 0.99 --max_step_size 8192 --port_increment 50
python3 run.py --envs_per_scenario 1 --wandb rlcc-new_simulator_v2 --wandb_run_name rtt_12_h_tgt_0.064_max_1.5_f_15_rl_3_act_0.2_alc_1_sl_2.5_lr_0.01_envs_1_qp_4k_bst_hl_4_relu_wmpup_50_wmplen_1024_mbatch_8192_msz_5000_max_no_2_1 --learning_rate 0.01 --agent ADPG --config rtt_adpg --architecture 12 --max_num_updates 1000 --action_multiplier_inc 0.2 --action_multiplier_dec 0.2 --agent_features action rtt_reward --target 0.064 --factor 15 --max_factor 1.5 --base_rtt 8192 --rollout_length 5 --action_loss_coeff 1 --loss_scale 2.5 --scenarios 4_1_qp 4_16_qp 8_1_qp 4_16_a2a_qp 8_8_a2a_qp 8_16_a2a_qp 32_1_qp --limit_flows --history_length 4 --leaky_relu_coeff 0.0 --warmup_updates 50 --warmup_length 1024  --max_batch_size 8192 --discount 0.99 --max_step_size 8192 --port_increment 50

# testing replacing LSTM with rnn or with GRU
python3 run.py --envs_per_scenario 1 --wandb rlcc-new_simulator_v2 --wandb_run_name rtt_12_h_tgt_0.064_max_1.5_f_15_rl_3_act_0.2_alc_1_sl_2.5_lr_0.01_envs_1_qp_4k_bst_gru_relu_wmpup_50_wmplen_1024_mbatch_8192_msz_5000_max_no_2_1 --learning_rate 0.01 --agent ADPG --config rtt_adpg --architecture 12 --max_num_updates 1000 --action_multiplier_inc 0.2 --action_multiplier_dec 0.2 --agent_features action rtt_reward --target 0.064 --factor 15 --max_factor 1.5 --base_rtt 8192 --rollout_length 5 --action_loss_coeff 1 --loss_scale 2.5 --scenarios 4_1_qp 4_16_qp 8_1_qp 4_16_a2a_qp 8_8_a2a_qp 8_16_a2a_qp 32_1_qp --limit_flows --use_rnn GRU --history_length 1 --leaky_relu_coeff 0.0 --warmup_updates 50 --warmup_length 1024  --max_batch_size 8192 --discount 0.99 --max_step_size 8192 --port_increment 50
python3 run.py --envs_per_scenario 1 --wandb rlcc-new_simulator_v2 --wandb_run_name rtt_12_h_tgt_0.064_max_1.5_f_15_rl_3_act_0.2_alc_1_sl_2.5_lr_0.01_envs_1_qp_4k_bst_rnn_relu_wmpup_50_wmplen_1024_mbatch_8192_msz_5000_max_no_2_1 --learning_rate 0.01 --agent ADPG --config rtt_adpg --architecture 12 --max_num_updates 1000 --action_multiplier_inc 0.2 --action_multiplier_dec 0.2 --agent_features action rtt_reward --target 0.064 --factor 15 --max_factor 1.5 --base_rtt 8192 --rollout_length 5 --action_loss_coeff 1 --loss_scale 2.5 --scenarios 4_1_qp 4_16_qp 8_1_qp 4_16_a2a_qp 8_8_a2a_qp 8_16_a2a_qp 32_1_qp --limit_flows --use_rnn RNN --history_length 1 --leaky_relu_coeff 0.0 --warmup_updates 50 --warmup_length 1024  --max_batch_size 8192 --discount 0.99 --max_step_size 8192 --port_increment 50

# seems that RNN and history length 4 work well for rtt so lets try for tele
python3 run.py --envs_per_scenario 1 --wandb rlcc-new_simulator_v2 --wandb_run_name tele_12_h_tgt_0.005_max_0.02_f_150_rl_3_act_0.2_alc_1_sl_5_lr_0.01_envs_1_qp_4k_bst_hl_4_relu_wmpup_50_wmplen_1024_mbatch_8192_msz_5000_max_no_2_1 --learning_rate 0.01 --qnorm 1 --agent ADPG --config telemetry_adpg --architecture 12 --max_num_updates 1000 --action_multiplier_inc 0.2 --action_multiplier_dec 0.2 --agent_features action qlength_reward --target 0.005 --factor 150 --max_factor 0.02 --base_rtt 8192 --rollout_length 3 --action_loss_coeff 1 --loss_scale 5 --scenarios 4_1_qp_t 8_1_qp_t 4_16_a2a_qp_t 8_8_a2a_qp_t 8_16_a2a_qp_t 32_1_qp_t --limit_flows --history_length 4 --leaky_relu_coeff 0.0 --warmup_updates 50 --warmup_length 1024  --max_batch_size 8192 --discount 0.99 --max_step_size 5000 --port_increment 50
python3 run.py --envs_per_scenario 1 --wandb rlcc-new_simulator_v2 --wandb_run_name tele_12_h_tgt_0.005_max_0.02_f_150_rl_3_act_0.2_alc_1_sl_5_lr_0.01_envs_1_qp_4k_bst_rnn_relu_wmpup_50_wmplen_1024_mbatch_8192_msz_5000_max_no_2_1 --learning_rate 0.01 --qnorm 1 --agent ADPG --config telemetry_adpg --architecture 12 --max_num_updates 1000 --action_multiplier_inc 0.2 --action_multiplier_dec 0.2 --agent_features action qlength_reward --target 0.005 --factor 150 --max_factor 0.02 --base_rtt 8192 --rollout_length 3 --action_loss_coeff 1 --loss_scale 5 --scenarios 4_1_qp_t 8_1_qp_t 4_16_a2a_qp_t 8_8_a2a_qp_t 8_16_a2a_qp_t 32_1_qp_t --limit_flows --use_rnn RNN --history_length 1 --leaky_relu_coeff 0.0 --warmup_updates 50 --warmup_length 1024  --max_batch_size 8192 --discount 0.99 --max_step_size 5000 --port_increment 50

# trying cwnd instead of rate
python3 run.py --envs_per_scenario 1 --wandb rlcc-new_simulator_v2 --wandb_run_name cwnd_12_h_tgt_0.064_max_1.5_f_15_rl_3_act_0.2_alc_1_sl_2.5_lr_0.01_envs_1_qp_4k_bst_hl_4_relu_wmpup_50_wmplen_1024_mbatch_8192_msz_5000_max_no_2_1 --learning_rate 0.01 --agent ADPG --config rtt_adpg --architecture 12 --max_num_updates 1000 --action_multiplier_inc 0.2 --action_multiplier_dec 0.2 --agent_features action rtt_reward --target 0.064 --factor 15 --max_factor 1.5 --base_rtt 8192 --rollout_length 5 --action_loss_coeff 1 --loss_scale 2.5 --scenarios 4_1_qp 4_16_qp 8_1_qp 4_16_a2a_qp 8_8_a2a_qp 8_16_a2a_qp 32_1_qp --limit_flows --history_length 4 --leaky_relu_coeff 0.0 --warmup_updates 50 --warmup_length 1024  --max_batch_size 8192 --discount 0.99 --max_step_size 8192 --port_increment 50
python3 run.py --envs_per_scenario 1 --wandb rlcc-new_simulator_v2 --wandb_run_name rtt_12_h_tgt_0.064_max_1.5_f_15_rl_3_act_0.2_alc_1_sl_5_lr_0.01_envs_1_qp_4k_bst_hl_4_relu_wmpup_50_wmplen_1024_mbatch_8192_msz_10000_max_no_2_1 --learning_rate 0.01 --agent ADPG --config rtt_adpg --architecture 12 --max_num_updates 1000 --action_multiplier_inc 0.2 --action_multiplier_dec 0.2 --agent_features action rtt_reward --target 0.064 --factor 15 --max_factor 1.5 --base_rtt 8192 --rollout_length 5 --action_loss_coeff 1 --loss_scale 5 --scenarios 4_1_qp 4_16_qp 8_1_qp 4_16_a2a_qp 8_8_a2a_qp 8_16_a2a_qp 32_1_qp --limit_flows --history_length 4 --leaky_relu_coeff 0.0 --warmup_updates 50 --warmup_length 1024  --max_batch_size 8192 --discount 0.99 --max_step_size 10000 --port_increment 50
python3 run.py --envs_per_scenario 1 --wandb rlcc-new_simulator_v2 --wandb_run_name rtt_12_12_h_tgt_0.064_max_1.5_f_15_rl_3_act_0.2_alc_1_sl_5_lr_0.01_envs_1_qp_4k_bst_hl_4_relu_wmpup_50_wmplen_1024_mbatch_8192_msz_10000_max_no_2_1 --learning_rate 0.01 --agent ADPG --config rtt_adpg --architecture 12 12 --max_num_updates 1000 --action_multiplier_inc 0.2 --action_multiplier_dec 0.2 --agent_features action rtt_reward --target 0.064 --factor 15 --max_factor 1.5 --base_rtt 8192 --rollout_length 5 --action_loss_coeff 1 --loss_scale 5 --scenarios 4_1_qp 4_16_qp 8_1_qp 4_16_a2a_qp 8_8_a2a_qp 8_16_a2a_qp 32_1_qp --limit_flows --history_length 4 --leaky_relu_coeff 0.0 --warmup_updates 50 --warmup_length 1024  --max_batch_size 8192 --discount 0.99 --max_step_size 10000 --port_increment 50

python3 run.py --envs_per_scenario 1 --wandb rlcc-new_simulator_v2 --wandb_run_name cwnd_12_h_tgt_0.064_max_1.5_f_15_rl_3_act_0.2_alc_1_sl_5_lr_0.01_envs_1_qp_4k_bst_hl_4_relu_wmpup_50_wmplen_1024_mbatch_8192_msz_5000_max_no_2_1 --learning_rate 0.01 --agent ADPG --config rtt_adpg --architecture 12 --max_num_updates 1000 --action_multiplier_inc 0.2 --action_multiplier_dec 0.2 --agent_features action rtt_reward --target 0.064 --factor 15 --max_factor 1.5 --base_rtt 8192 --rollout_length 5 --action_loss_coeff 1 --loss_scale 5 --scenarios 4_1_qp 4_16_qp 8_1_qp 4_16_a2a_qp 8_8_a2a_qp 8_16_a2a_qp 32_1_qp --limit_flows --history_length 4 --leaky_relu_coeff 0.0 --warmup_updates 50 --warmup_length 1024  --max_batch_size 8192 --discount 0.99 --max_step_size 5000 --port_increment 50

# return to rtt
python3 run.py --envs_per_scenario 1 --wandb rlcc-new_simulator_v2 --wandb_run_name rtt_12_12_h_tgt_0.064_max_1.5_f_15_rl_3_act_0.2_alc_1_sl_5_lr_0.01_envs_1_qp_4k_bst_hl_2_relu_wmpup_50_wmplen_1024_mbatch_8192_msz_10000_max_no_2_1_64_1 --learning_rate 0.01 --agent ADPG --config rtt_adpg --architecture 12 12 --max_num_updates 1000 --action_multiplier_inc 0.2 --action_multiplier_dec 0.2 --agent_features action rtt_reward --target 0.064 --factor 15 --max_factor 1.5 --base_rtt 8192 --rollout_length 5 --action_loss_coeff 1 --loss_scale 5 --scenarios 4_1_qp 4_16_qp 8_1_qp  8_8_qp 4_16_a2a_qp 8_8_a2a_qp 8_16_a2a_qp 32_1_qp 64_1_qp --limit_flows --history_length 2 --leaky_relu_coeff 0.0 --warmup_updates 50 --warmup_length 1024  --max_batch_size 8192 --discount 0.99 --max_step_size 10000 --port_increment 50
python3 run.py --envs_per_scenario 1 --wandb rlcc-new_simulator_v2 --wandb_run_name rtt_12_h_tgt_0.064_max_1.5_f_15_rl_3_act_0.2_alc_1_sl_5_lr_0.01_envs_1_qp_4k_bst_hl_2_relu_wmpup_50_wmplen_1024_mbatch_8192_msz_10000_max_no_2_1_64_1 --learning_rate 0.01 --agent ADPG --config rtt_adpg --architecture 12 --max_num_updates 1000 --action_multiplier_inc 0.2 --action_multiplier_dec 0.2 --agent_features action rtt_reward --target 0.064 --factor 15 --max_factor 1.5 --base_rtt 8192 --rollout_length 5 --action_loss_coeff 1 --loss_scale 5 --scenarios 4_1_qp 4_16_qp 8_1_qp  8_8_qp 4_16_a2a_qp 8_8_a2a_qp 8_16_a2a_qp 32_1_qp 64_1_qp --limit_flows --history_length 2 --leaky_relu_coeff 0.0 --warmup_updates 50 --warmup_length 1024  --max_batch_size 8192 --discount 0.99 --max_step_size 10000 --port_increment 50
# cwnd
python3 run.py --envs_per_scenario 1 --wandb rlcc-new_simulator_v2 --wandb_run_name cwnd_12_12_h_tgt_0.064_max_1.5_f_15_rl_3_act_0.2_alc_1_sl_5_lr_0.01_envs_1_qp_4k_bst_hl_2_relu_wmpup_50_wmplen_1024_mbatch_8192_msz_10000_max_no_2_1_64_1 --learning_rate 0.01 --agent ADPG --config rtt_adpg --architecture 12 12 --max_num_updates 1000 --action_multiplier_inc 0.2 --action_multiplier_dec 0.2 --agent_features action rtt_reward --target 0.064 --factor 15 --max_factor 1.5 --base_rtt 8192 --rollout_length 5 --action_loss_coeff 1 --loss_scale 5 --scenarios 4_1_qp 4_16_qp 8_1_qp  8_8_qp 4_16_a2a_qp 8_8_a2a_qp 8_16_a2a_qp 32_1_qp 64_1_qp --limit_flows --history_length 2 --leaky_relu_coeff 0.0 --warmup_updates 50 --warmup_length 1024  --max_batch_size 8192 --discount 0.99 --max_step_size 10000 --port_increment 50
python3 run.py --envs_per_scenario 1 --wandb rlcc-new_simulator_v2 --wandb_run_name cwnd_12_h_tgt_0.064_max_1.5_f_15_rl_3_act_0.2_alc_1_sl_5_lr_0.01_envs_1_qp_4k_bst_hl_2_relu_wmpup_50_wmplen_1024_mbatch_8192_msz_10000_max_no_2_1_64_1 --learning_rate 0.01 --agent ADPG --config rtt_adpg --architecture 12 --max_num_updates 1000 --action_multiplier_inc 0.2 --action_multiplier_dec 0.2 --agent_features action rtt_reward --target 0.064 --factor 15 --max_factor 1.5 --base_rtt 8192 --rollout_length 5 --action_loss_coeff 1 --loss_scale 5 --scenarios 4_1_qp 4_16_qp 8_1_qp  8_8_qp 4_16_a2a_qp 8_8_a2a_qp 8_16_a2a_qp 32_1_qp 64_1_qp --limit_flows --history_length 2 --leaky_relu_coeff 0.0 --warmup_updates 50 --warmup_length 1024  --max_batch_size 8192 --discount 0.99 --max_step_size 10000 --port_increment 50


# try dynamic target for paper
python3 run.py --envs_per_scenario 1 --wandb rlcc-new_simulator_v2 --wandb_run_name rtt_12_12_h_tgt_0_max_0_f_4_rl_3_act_0.2_alc_1_sl_5_lr_0.01_envs_1_qp_4k_bst_hl_2_relu_wmpup_50_wmplen_1024_mbatch_8192_msz_10000_dynmac_tgt --learning_rate 0.01 --agent ADPG --config rtt_adpg --architecture 12 12 --max_num_updates 1000 --action_multiplier_inc 0.2 --action_multiplier_dec 0.2 --agent_features action rtt_reward --target 0.064 --factor 4 --max_factor 0 --base_rtt 8192 --rollout_length 5 --action_loss_coeff 1 --loss_scale 5 --scenarios 4_1_qp 4_16_qp 8_1_qp  8_8_qp 32_1_qp 64_1_qp --limit_flows --history_length 2 --leaky_relu_coeff 0.0 --warmup_updates 50 --warmup_length 1024  --max_batch_size 8192 --discount 0.99 --max_step_size 10000 --use_dynamic_target --port_increment 50
python3 run.py --envs_per_scenario 1 --wandb rlcc-new_simulator_v2 --wandb_run_name rtt_12_12_h_tgt_0_max_0_f_4_rl_3_act_0.2_alc_1_sl_5_lr_0.01_envs_1_qp_4k_bst_hl_2_relu_wmpup_50_wmplen_1024_mbatch_8192_msz_10000_dynmac_tgtsgnchange --learning_rate 0.01 --agent ADPG --config rtt_adpg --architecture 12 12 --max_num_updates 1000 --action_multiplier_inc 0.2 --action_multiplier_dec 0.2 --agent_features action rtt_reward --target 0.064 --factor 4 --max_factor 0 --base_rtt 8192 --rollout_length 5 --action_loss_coeff 1 --loss_scale 5 --scenarios 4_1_qp 4_16_qp 8_1_qp  8_8_qp 32_1_qp 64_1_qp --limit_flows --history_length 2 --leaky_relu_coeff 0.0 --warmup_updates 50 --warmup_length 1024  --max_batch_size 8192 --discount 0.99 --max_step_size 10000 --use_dynamic_target --port_increment 50
python3 run.py --envs_per_scenario 1 --wandb rlcc-new_simulator_v2 --wandb_run_name rtt_12_12_h_tgt_2_max_0_f_4_rl_3_act_0.2_alc_1_sl_5_lr_0.01_envs_1_qp_4k_bst_hl_2_relu_wmpup_50_wmplen_1024_mbatch_8192_msz_10000_dynmac_tgtsgnchange --learning_rate 0.01 --agent ADPG --config rtt_adpg --architecture 12 12 --max_num_updates 1000 --action_multiplier_inc 0.2 --action_multiplier_dec 0.2 --agent_features action rtt_reward --target 2 --factor 4 --max_factor 0 --base_rtt 8192 --rollout_length 5 --action_loss_coeff 1 --loss_scale 5 --scenarios 4_1_qp 4_16_qp 8_1_qp  8_8_qp 32_1_qp 64_1_qp 4_16_a2a_qp 8_8_a2a_qp 8_16_a2a_qp --limit_flows --history_length 2 --leaky_relu_coeff 0.0 --warmup_updates 50 --warmup_length 1024  --max_batch_size 8192 --discount 0.99 --max_step_size 10000 --port_increment 50
python3 run.py --envs_per_scenario 1 --wandb rlcc-new_simulator_v2 --wandb_run_name rtt_12_12_h_tgt_2_max_0_f_1_rl_3_act_0.2_alc_1_sl_5_lr_0.01_envs_1_qp_4k_bst_hl_2_relu_wmpup_50_wmplen_1024_mbatch_8192_msz_10000_dynmac_tgtsgnchange --learning_rate 0.01 --agent ADPG --config rtt_adpg --architecture 12 12 --max_num_updates 1000 --action_multiplier_inc 0.2 --action_multiplier_dec 0.2 --agent_features action rtt_reward --target 2 --factor 1 --max_factor 0 --base_rtt 8192 --rollout_length 5 --action_loss_coeff 1 --loss_scale 5 --scenarios 4_1_qp 4_16_qp 8_1_qp  8_8_qp 32_1_qp 64_1_qp 4_16_a2a_qp 8_8_a2a_qp 8_16_a2a_qp --limit_flows --history_length 2 --leaky_relu_coeff 0.0 --warmup_updates 50 --warmup_length 1024  --max_batch_size 8192 --discount 0.99 --max_step_size 10000 --port_increment 50
python3 run.py --envs_per_scenario 1 --wandb rlcc-new_simulator_v2 --wandb_run_name rtt_12_12_h_tgt_1_max_0_f_1_rl_3_act_0.2_alc_1_sl_5_lr_0.01_envs_1_qp_4k_bst_hl_2_relu_wmpup_50_wmplen_1024_mbatch_8192_msz_10000_dynmac_tgtsgnchange --learning_rate 0.01 --agent ADPG --config rtt_adpg --architecture 12 12 --max_num_updates 1000 --action_multiplier_inc 0.2 --action_multiplier_dec 0.2 --agent_features action rtt_reward --target 1 --factor 1 --max_factor 0 --base_rtt 8192 --rollout_length 5 --action_loss_coeff 1 --loss_scale 5 --scenarios 4_1_qp 4_16_qp 8_1_qp  8_8_qp 32_1_qp 64_1_qp 4_16_a2a_qp 8_8_a2a_qp 8_16_a2a_qp --limit_flows --history_length 2 --leaky_relu_coeff 0.0 --warmup_updates 50 --warmup_length 1024  --max_batch_size 8192 --discount 0.99 --max_step_size 10000 --port_increment 50
python3 run.py --envs_per_scenario 1 --wandb rlcc-new_simulator_v2 --wandb_run_name rtt_12_12_h_tgt_0.5_max_0_f_1_rl_3_act_0.2_alc_1_sl_5_lr_0.01_envs_1_qp_4k_bst_hl_2_relu_wmpup_50_wmplen_1024_mbatch_8192_msz_10000_dynmac_tgtsgnchange --learning_rate 0.01 --agent ADPG --config rtt_adpg --architecture 12 12 --max_num_updates 1000 --action_multiplier_inc 0.2 --action_multiplier_dec 0.2 --agent_features action rtt_reward --target 1 --factor 1 --max_factor 0 --base_rtt 8192 --rollout_length 5 --action_loss_coeff 1 --loss_scale 5 --scenarios 8_8_qp 32_1_qp 64_1_qp 4_16_a2a_qp 8_8_a2a_qp 8_16_a2a_qp --limit_flows --history_length 2 --leaky_relu_coeff 0.0 --warmup_updates 50 --warmup_length 1024  --max_batch_size 8192 --discount 0.99 --max_step_size 10000 --port_increment 50
python3 run.py --envs_per_scenario 1 --wandb rlcc-new_simulator_v2 --wandb_run_name rtt_12_12_h_tgt_0.5_max_0_f_1_rl_3_act_0.2_alc_1_sl_5_lr_0.01_envs_1_qp_4k_bst_hl_2_relu_wmpup_50_wmplen_1024_mbatch_8192_msz_10000_dynmac_v2_fix --learning_rate 0.01 --agent ADPG --config rtt_adpg --architecture 12 12 --max_num_updates 1000 --action_multiplier_inc 0.2 --action_multiplier_dec 0.2 --agent_features action rtt_reward --target 0.5 --factor 1 --max_factor 0 --base_rtt 8192 --rollout_length 5 --action_loss_coeff 1 --loss_scale 5 --scenarios 4_1_qp 4_16_qp 8_8_qp 32_1_qp 64_1_qp 32_8_qp --limit_flows --history_length 2 --leaky_relu_coeff 0.0 --warmup_updates 50 --warmup_length 1024  --max_batch_size 8192 --discount 0.99 --max_step_size 10000 --port_increment 50
python3 run.py --envs_per_scenario 1 --wandb rlcc-new_simulator_v2 --wandb_run_name rtt_12_12_h_tgt_0.25_max_0_f_1_rl_3_act_0.2_alc_1_sl_5_lr_0.01_envs_1_qp_4k_bst_hl_2_relu_wmpup_50_wmplen_1024_mbatch_8192_msz_10000_dynmac_v2 --learning_rate 0.01 --agent ADPG --config rtt_adpg --architecture 12 12 --max_num_updates 1000 --action_multiplier_inc 0.2 --action_multiplier_dec 0.2 --agent_features action rtt_reward --target 0.25 --factor 1 --max_factor 0 --base_rtt 8192 --rollout_length 5 --action_loss_coeff 1 --loss_scale 5 --scenarios 4_1_qp 4_16_qp 8_8_qp 32_1_qp 64_1_qp 32_8_qp --limit_flows --history_length 2 --leaky_relu_coeff 0.0 --warmup_updates 50 --warmup_length 1024  --max_batch_size 8192 --discount 0.99 --max_step_size 10000 --port_increment 50
python3 run.py --envs_per_scenario 1 --wandb rlcc-new_simulator_v2 --wandb_run_name rtt_12_12_h_tgt_0.1_max_0_f_1_rl_3_act_0.2_alc_1_sl_5_lr_0.01_envs_1_qp_4k_bst_hl_2_relu_wmpup_50_wmplen_1024_mbatch_8192_msz_10000_dynmac_v2 --learning_rate 0.01 --agent ADPG --config rtt_adpg --architecture 12 12 --max_num_updates 1000 --action_multiplier_inc 0.2 --action_multiplier_dec 0.2 --agent_features action rtt_reward --target 0.1 --factor 1 --max_factor 0 --base_rtt 8192 --rollout_length 5 --action_loss_coeff 1 --loss_scale 5 --scenarios 4_1_qp 4_16_qp 8_8_qp 32_1_qp 64_1_qp 32_8_qp --limit_flows --history_length 2 --leaky_relu_coeff 0.0 --warmup_updates 50 --warmup_length 1024  --max_batch_size 8192 --discount 0.99 --max_step_size 10000 --port_increment 50

# playing with relative delay
python3 run.py --envs_per_scenario 1 --wandb rlcc-new_simulator_v3 --wandb_run_name relative_rtt_12_12_h_tgt_0.064_max_4_f_15_rl_3_act_0.2_alc_1_sl_5_lr_0.01_envs_1_qp_4k_bst_hl_2_relu_wmpup_50_wmplen_1024_mbatch_8k_msz_6k_max_no_2_1_64_1 --learning_rate 0.01 --agent ADPG --config rtt_adpg --architecture 12 12 --max_num_updates 1000 --action_multiplier_inc 0.2 --action_multiplier_dec 0.2 --agent_features action relative_rtt_reward --target 0.064 --factor 15 --max_factor 4 --base_rtt 8192 --rollout_length 5 --action_loss_coeff 1 --loss_scale 5 --scenarios 4_1_qp 4_16_qp 8_1_qp  8_8_qp 4_16_a2a_qp 8_8_a2a_qp 8_16_a2a_qp 32_1_qp 64_1_qp --limit_flows --history_length 2 --leaky_relu_coeff 0.0 --warmup_updates 50 --warmup_length 1024  --max_batch_size 8192 --discount 0.99 --max_step_size 6000 --port_increment 50

# trying qp + ip mode

python3 run.py --envs_per_scenario 1 --wandb rlcc-new_simulator_v3 --wandb_run_name rtt_12_12_h_tgt_0.064_max_1.5_f_15_rl_3_act_0.2_alc_1_sl_5_lr_0.01_envs_1_qp_ip_4k_bst_hl_2_relu_wmpup_50_wmplen_1024_mbatch_8192_msz_10000_max_no_2_1_64_1 --learning_rate 0.01 --agent ADPG --config rtt_adpg --architecture 12 12 --max_num_updates 1000 --action_multiplier_inc 0.2 --action_multiplier_dec 0.2 --agent_features action rtt_reward --target 0.064 --factor 15 --max_factor 1.5 --base_rtt 8192 --rollout_length 5 --action_loss_coeff 1 --loss_scale 5 --scenarios 4_1_qp 4_16_qp 8_1_qp  8_8_qp 4_16_a2a_qp 8_8_a2a_qp 8_16_a2a_qp 32_1_qp 64_1_qp 4_1024_ip 8_1024_ip 8_512_ip  --limit_flows --history_length 2 --leaky_relu_coeff 0.0 --warmup_updates 50 --warmup_length 1024  --max_batch_size 8192 --discount 0.99 --max_step_size 10000 --port_increment 50
python3 run.py --envs_per_scenario 1 --wandb rlcc-new_simulator_v3 --wandb_run_name rtt_12_12_h_tgt_0.064_max_1.5_f_15_rl_3_act_0.2_alc_1_sl_5_lr_0.01_envs_1_qp_ip_4k_bst_hl_2_relu_wmpup_50_wmplen_1024_mbatch_8192_msz_3000_max_no_2_1_64_1 --learning_rate 0.01 --agent ADPG --config rtt_adpg --architecture 12 12 --max_num_updates 1000 --action_multiplier_inc 0.2 --action_multiplier_dec 0.2 --agent_features action rtt_reward --target 0.064 --factor 15 --max_factor 1.5 --base_rtt 8192 --rollout_length 5 --action_loss_coeff 1 --loss_scale 5 --scenarios 4_1_qp 4_16_qp 8_1_qp  8_8_qp 4_16_a2a_qp 8_8_a2a_qp 8_16_a2a_qp 32_1_qp 64_1_qp 4_1024_ip 8_1024_ip 8_512_ip  --limit_flows --history_length 2 --leaky_relu_coeff 0.0 --warmup_updates 50 --warmup_length 1024  --max_batch_size 8192 --discount 0.99 --max_step_size 3000 --port_increment 50
python3 run.py --envs_per_scenario 1 --wandb rlcc-new_simulator_v3 --wandb_run_name rtt_12_12_h_tgt_0.064_max_1.5_f_15_rl_3_act_0.2_alc_1_sl_5_lr_0.01_envs_1_qp_ip_4k_bst_hl_2_relu_wmpup_50_wmplen_1024_mbatch_8192_msz_3000_max_no_2_1_64_1_v2 --learning_rate 0.01 --agent ADPG --config rtt_adpg --architecture 12 12 --max_num_updates 1000 --action_multiplier_inc 0.2 --action_multiplier_dec 0.2 --agent_features action rtt_reward --target 0.064 --factor 15 --max_factor 1.5 --base_rtt 8192 --rollout_length 5 --action_loss_coeff 1 --loss_scale 5 --scenarios 4_1_qp 4_16_qp 8_1_qp  8_8_qp 4_16_a2a_qp 8_8_a2a_qp 8_16_a2a_qp 32_1_qp 64_1_qp 4_1024_ip 8_1024_ip 8_512_ip  --limit_flows --history_length 2 --leaky_relu_coeff 0.0 --warmup_updates 50 --warmup_length 1024  --max_batch_size 8192 --discount 0.99 --max_step_size 3000 --port_increment 50
python3 run.py --envs_per_scenario 1 --wandb rlcc-new_simulator_v3 --wandb_run_name rtt_12_12_h_tgt_0.064_max_1.5_f_15_rl_3_act_0.2_alc_1_sl_5_lr_0.01_envs_1_qp_ip_4k_bst_hl_2_relu_wmpup_50_wmplen_1024_mbatch_8192_msz_2000_max_no_2_1_64_1 --learning_rate 0.01 --agent ADPG --config rtt_adpg --architecture 12 12 --max_num_updates 1000 --action_multiplier_inc 0.2 --action_multiplier_dec 0.2 --agent_features action rtt_reward --target 0.064 --factor 15 --max_factor 1.5 --base_rtt 8192 --rollout_length 5 --action_loss_coeff 1 --loss_scale 5 --scenarios 4_1_qp 4_16_qp 8_1_qp 8_8_qp 4_16_a2a_qp 8_8_a2a_qp 8_16_a2a_qp 32_1_qp 64_1_qp 4_1024_ip 8_1024_ip 8_512_ip  --limit_flows --history_length 2 --leaky_relu_coeff 0.0 --warmup_updates 50 --warmup_length 1024  --max_batch_size 8192 --discount 0.99 --max_step_size 2000 --port_increment 50

# trying qp-ip with higher target or lower increase
python3 run.py --envs_per_scenario 1 --wandb rlcc-new_simulator_v3 --wandb_run_name rtt_12_12_h_tgt_0.1_max_1.5_f_15_rl_3_act_0.2_alc_1_sl_5_lr_0.01_envs_1_qp_ip_4k_bst_hl_2_relu_wmpup_50_wmplen_1024_mbatch_8192_msz_2000_max_no_2_1_64_1 --learning_rate 0.01 --agent ADPG --config rtt_adpg --architecture 12 12 --max_num_updates 1000 --action_multiplier_inc 0.2 --action_multiplier_dec 0.2 --agent_features action rtt_reward --target 0.1 --factor 15 --max_factor 1.5 --base_rtt 8192 --rollout_length 5 --action_loss_coeff 1 --loss_scale 5 --scenarios 4_1_qp 4_16_qp 8_1_qp 8_8_qp 4_16_a2a_qp 8_8_a2a_qp 8_16_a2a_qp 32_1_qp 64_1_qp 4_1024_ip 8_1024_ip 8_512_ip  --limit_flows --history_length 2 --leaky_relu_coeff 0.0 --warmup_updates 50 --warmup_length 1024  --max_batch_size 8192 --discount 0.99 --max_step_size 2000 --port_increment 50
python3 run.py --envs_per_scenario 1 --wandb rlcc-new_simulator_v3 --wandb_run_name rtt_12_12_h_tgt_0.06_max_1.5_f_15_rl_3_act_0.1_inc_alc_1_sl_5_lr_0.01_envs_1_qp_ip_4k_bst_hl_2_relu_wmpup_50_wmplen_1024_mbatch_8192_msz_2000_max_no_2_1_64_1 --learning_rate 0.01 --agent ADPG --config rtt_adpg --architecture 12 12 --max_num_updates 1000 --action_multiplier_inc 0.1 --action_multiplier_dec 0.2 --agent_features action rtt_reward --target 0.06 --factor 15 --max_factor 1.5 --base_rtt 8192 --rollout_length 5 --action_loss_coeff 1 --loss_scale 5 --scenarios 4_1_qp 4_16_qp 8_1_qp 8_8_qp 4_16_a2a_qp 8_8_a2a_qp 8_16_a2a_qp 32_1_qp 64_1_qp 4_1024_ip 8_1024_ip 8_512_ip  --limit_flows --history_length 2 --leaky_relu_coeff 0.0 --warmup_updates 50 --warmup_length 1024  --max_batch_size 8192 --discount 0.99 --max_step_size 2000 --port_increment 50
# trying to reduce factor size
python3 run.py --envs_per_scenario 1 --wandb rlcc-new_simulator_v3 --wandb_run_name rtt_ip_12_12_h_tgt_0.1_max_1.5_f_5_rl_3_act_0.2_alc_1_sl_1_lr_0.01_envs_1_qp_ip_4k_bst_hl_2_relu_wmpup_50_wmplen_1024_mbatch_8192_msz_2000_max_no_2_1_64_1 --learning_rate 0.01 --agent ADPG --config rtt_adpg --architecture 12 12 --max_num_updates 1000 --action_multiplier_inc 0.2 --action_multiplier_dec 0.2 --agent_features action rtt_reward --target 0.1 --factor 5 --max_factor 1.5 --base_rtt 8192 --rollout_length 5 --action_loss_coeff 1 --loss_scale 1 --scenarios 4_1_qp 4_16_qp 8_1_qp 8_8_qp 4_16_a2a_qp 8_8_a2a_qp 8_16_a2a_qp 32_1_qp 64_1_qp 4_1024_ip 8_1024_ip 8_512_ip  --limit_flows --history_length 2 --leaky_relu_coeff 0.0 --warmup_updates 50 --warmup_length 1024  --max_batch_size 8192 --discount 0.99 --max_step_size 2000 --port_increment 50
python3 run.py --envs_per_scenario 1 --wandb rlcc-new_simulator_v3 --wandb_run_name rtt_ip_12_12_h_tgt_0.1_max_1.5_f_5_rl_3_act_0.2_alc_1_sl_1_lr_0.01_envs_1_qp_ip_4k_bst_hl_2_relu_wmpup_50_wmplen_1024_mbatch_8192_msz_2000_max_no_2_1_64_1_g_0.97 --learning_rate 0.01 --agent ADPG --config rtt_adpg --architecture 12 12 --max_num_updates 1000 --action_multiplier_inc 0.2 --action_multiplier_dec 0.2 --agent_features action rtt_reward --target 0.1 --factor 5 --max_factor 1.5 --base_rtt 8192 --rollout_length 5 --action_loss_coeff 1 --loss_scale 1 --scenarios 4_1_qp 4_16_qp 8_1_qp 8_8_qp 4_16_a2a_qp 8_8_a2a_qp 8_16_a2a_qp 32_1_qp 64_1_qp 4_1024_ip 8_1024_ip 8_512_ip  --limit_flows --history_length 2 --leaky_relu_coeff 0.0 --warmup_updates 50 --warmup_length 1024  --max_batch_size 8192 --discount 0.97 --max_step_size 2000 --port_increment 50
python3 run.py --envs_per_scenario 1 --wandb rlcc-new_simulator_v3 --wandb_run_name rtt_ip_12_12_h_tgt_0.1_max_1.5_f_5_rl_3_act_0.2_alc_1_sl_5_lr_0.01_envs_1_qp_ip_4k_bst_hl_2_relu_wmpup_50_wmplen_1024_mbatch_8192_msz_2000_max_no_2_1_64_1 --learning_rate 0.01 --agent ADPG --config rtt_adpg --architecture 12 12 --max_num_updates 1000 --action_multiplier_inc 0.2 --action_multiplier_dec 0.2 --agent_features action rtt_reward --target 0.1 --factor 5 --max_factor 1.5 --base_rtt 8192 --rollout_length 5 --action_loss_coeff 1 --loss_scale 5 --scenarios 4_1_qp 4_16_qp 8_1_qp 8_8_qp 4_16_a2a_qp 8_8_a2a_qp 8_16_a2a_qp 32_1_qp 64_1_qp 4_1024_ip 8_1024_ip 8_512_ip  --limit_flows --history_length 2 --leaky_relu_coeff 0.0 --warmup_updates 50 --warmup_length 1024  --max_batch_size 8192 --discount 0.99 --max_step_size 2000 --port_increment 50
# back to telemetry normalized to 2 MiB buffer
python3 run.py --envs_per_scenario 1 --wandb rlcc-new_simulator_v3 --wandb_run_name tele_12_12_h_tgt_0.01_max_0.01_f_10_1_rl_5_act_0.2_alc_1_sl_1_lr_0.01_envs_1_qp_4k_bst_lstm_relu_wmpup_50_wmplen_1024_mbatch_8192_msz_4000_g_0.99 --learning_rate 0.01 --agent ADPG --config telemetry_adpg --architecture 12 12 --max_num_updates 1000 --action_multiplier_inc 0.2 --action_multiplier_dec 0.2 --agent_features action qlength_reward --target 0.01 --factor 10 --max_factor 0.01  --rollout_length 5 --action_loss_coeff 1 --loss_scale 1 --scenarios 2_1_qp_t 4_1_qp_t 8_1_qp_t 4_16_a2a_qp_t 8_8_a2a_qp_t 8_16_a2a_qp_t 32_1_qp_t  --limit_flows --history_length 1 --leaky_relu_coeff 0.0 --warmup_updates 50 --warmup_length 1024 --max_batch_size 4000 --discount 0.99 --max_step_size 4000 --port_increment 50
python3 run.py --envs_per_scenario 1 --wandb rlcc-new_simulator_v3 --wandb_run_name tele_12_12_h_tgt_0.01_max_0.01_f_100_1_rl_5_act_0.2_alc_1_sl_1_lr_0.01_envs_1_qp_4k_bst_2_hl_relu_wmpup_50_wmplen_1024_mbatch_8192_msz_4000_g_0.99 --learning_rate 0.01 --agent ADPG --config telemetry_adpg --architecture 12 12 --max_num_updates 1000 --action_multiplier_inc 0.2 --action_multiplier_dec 0.2 --agent_features action qlength_reward --target 0.01 --factor 100 --max_factor 0.01  --rollout_length 5 --action_loss_coeff 1 --loss_scale 1 --scenarios 2_1_qp_t 4_1_qp_t 8_1_qp_t 4_16_a2a_qp_t 8_8_a2a_qp_t 8_16_a2a_qp_t 32_1_qp_t  --limit_flows --history_length 2 --leaky_relu_coeff 0.0 --warmup_updates 50 --warmup_length 1024 --max_batch_size 4000 --discount 0.99 --max_step_size 4000 --port_increment 50
python3 run.py --envs_per_scenario 1 --wandb rlcc-new_simulator_v3 --wandb_run_name tele_12_12_h_tgt_0.005_max_0.01_f_100_1_rl_5_act_0.2_alc_1_sl_1_lr_0.01_envs_1_qp_4k_bst_2_hl_relu_wmpup_50_wmplen_1024_mbatch_8192_msz_4000_g_0.99 --learning_rate 0.01 --agent ADPG --config telemetry_adpg --architecture 12 12 --max_num_updates 1000 --action_multiplier_inc 0.2 --action_multiplier_dec 0.2 --agent_features action qlength_reward --target 0.005 --factor 100 --max_factor 0.01  --rollout_length 5 --action_loss_coeff 1 --loss_scale 1 --scenarios 2_1_qp_t 4_1_qp_t 8_1_qp_t 4_16_a2a_qp_t 8_8_a2a_qp_t 8_16_a2a_qp_t 32_1_qp_t  --limit_flows --history_length 2 --leaky_relu_coeff 0.0 --warmup_updates 50 --warmup_length 1024 --max_batch_size 4000 --discount 0.99 --max_step_size 4000 --port_increment 50
python3 run.py --envs_per_scenario 1 --wandb rlcc-new_simulator_v3 --wandb_run_name tele_12_12_h_tgt_0.05_max_0.01_f_100_1_rl_5_act_0.2_alc_1_sl_1_lr_0.01_envs_1_qp_4k_bst_2_hl_relu_wmpup_50_wmplen_1024_mbatch_8192_msz_4000_g_0.99 --learning_rate 0.01 --agent ADPG --config telemetry_adpg --architecture 12 12 --max_num_updates 1000 --action_multiplier_inc 0.2 --action_multiplier_dec 0.2 --agent_features action qlength_reward --target 0.05 --factor 100 --max_factor 0.01  --rollout_length 5 --action_loss_coeff 1 --loss_scale 1 --scenarios 2_1_qp_t 4_1_qp_t 8_1_qp_t 4_16_a2a_qp_t 8_8_a2a_qp_t 8_16_a2a_qp_t 32_1_qp_t  --limit_flows --history_length 2 --leaky_relu_coeff 0.0 --warmup_updates 50 --warmup_length 1024 --max_batch_size 4000 --discount 0.99 --max_step_size 4000 --port_increment 50
# telemetry issue with too large factor
python3 run.py --envs_per_scenario 1 --wandb rlcc-new_simulator_v3 --wandb_run_name tele_12_12_h_tgt_0.05_max_0.01_f_10_1_rl_5_act_0.2_alc_1_sl_1_lr_0.01_envs_1_qp_4k_bst_2_hl_lrelu_0.1_wmpup_50_wmplen_1024_mbatch_8192_msz_4000_g_0.99 --learning_rate 0.01 --agent ADPG --config telemetry_adpg --architecture 12 12 --max_num_updates 2000 --action_multiplier_inc 0.2 --action_multiplier_dec 0.2 --agent_features action qlength_reward --target 0.05 --factor 10 --max_factor 0.01  --rollout_length 5 --action_loss_coeff 1 --loss_scale 1 --scenarios 2_1_qp_t 4_1_qp_t 8_1_qp_t 4_16_a2a_qp_t 8_8_a2a_qp_t 8_16_a2a_qp_t 32_1_qp_t  --limit_flows --history_length 2 --leaky_relu_coeff 0.1 --warmup_updates 50 --warmup_length 1024 --max_batch_size 4000 --discount 0.99 --max_step_size 4000 --port_increment 50
python3 run.py --envs_per_scenario 1 --wandb rlcc-new_simulator_v3 --wandb_run_name tele_12_12_h_tgt_0.1_max_0.01_f_10_1_rl_5_act_0.2_alc_1_sl_1_lr_0.01_envs_1_qp_4k_bst_2_hl_lrelu_0.1_wmpup_50_wmplen_1024_mbatch_8192_msz_4000_g_0.99 --learning_rate 0.01 --agent ADPG --config telemetry_adpg --architecture 12 12 --max_num_updates 2000 --action_multiplier_inc 0.2 --action_multiplier_dec 0.2 --agent_features action qlength_reward --target 0.1 --factor 10 --max_factor 0.01  --rollout_length 5 --action_loss_coeff 1 --loss_scale 1 --scenarios 2_1_qp_t 4_1_qp_t 8_1_qp_t 4_16_a2a_qp_t 8_8_a2a_qp_t 8_16_a2a_qp_t 32_1_qp_t  --limit_flows --history_length 2 --leaky_relu_coeff 0.1 --warmup_updates 50 --warmup_length 1024 --max_batch_size 4000 --discount 0.99 --max_step_size 4000 --port_increment 50
python3 run.py --envs_per_scenario 1 --wandb rlcc-new_simulator_v3 --wandb_run_name tele_12_12_h_tgt_0.01_max_0.01_f_10_1_rl_5_act_0.2_alc_1_sl_1_lr_0.01_envs_1_qp_4k_bst_2_hl_lrelu_0.1_wmpup_50_wmplen_1024_mbatch_8192_msz_4000_g_0.99 --learning_rate 0.01 --agent ADPG --config telemetry_adpg --architecture 12 12 --max_num_updates 2000 --action_multiplier_inc 0.2 --action_multiplier_dec 0.2 --agent_features action qlength_reward --target 0.01 --factor 10 --max_factor 0.01  --rollout_length 5 --action_loss_coeff 1 --loss_scale 1 --scenarios 2_1_qp_t 4_1_qp_t 8_1_qp_t 4_16_a2a_qp_t 8_8_a2a_qp_t 8_16_a2a_qp_t 32_1_qp_t  --limit_flows --history_length 2 --leaky_relu_coeff 0.1 --warmup_updates 50 --warmup_length 1024 --max_batch_size 4000 --discount 0.99 --max_step_size 4000 --port_increment 50
python3 run.py --envs_per_scenario 1 --wandb rlcc-new_simulator_v3 --wandb_run_name tele_12_12_h_tgt_0.001_max_0.01_f_80_1_rl_5_act_0.2_alc_1_sl_1_lr_0.01_envs_1_qp_4k_bst_2_hl_lrelu_0.1_wmpup_50_wmplen_1024_mbatch_8192_msz_4000_g_0.99 --learning_rate 0.01 --agent ADPG --config telemetry_adpg --architecture 12 12 --max_num_updates 2000 --action_multiplier_inc 0.2 --action_multiplier_dec 0.2 --agent_features action qlength_reward --target 0.001 --factor 80 --max_factor 0.01  --rollout_length 5 --action_loss_coeff 1 --loss_scale 1 --scenarios 2_1_qp_t 4_1_qp_t 8_1_qp_t 4_16_a2a_qp_t 8_8_a2a_qp_t 8_16_a2a_qp_t 32_1_qp_t  --limit_flows --history_length 2 --leaky_relu_coeff 0.1 --warmup_updates 50 --warmup_length 1024 --max_batch_size 4000 --discount 0.99 --max_step_size 4000 --port_increment 50
python3 run.py --envs_per_scenario 1 --wandb rlcc-new_simulator_v3 --wandb_run_name tele_12_12_h_tgt_0.005_max_0.01_f_80_1_rl_5_act_0.2_alc_1_sl_1_lr_0.01_envs_1_qp_4k_bst_2_hl_lrelu_0.1_wmpup_50_wmplen_1024_mbatch_8192_msz_4000_g_0.99 --learning_rate 0.01 --agent ADPG --config telemetry_adpg --architecture 12 12 --max_num_updates 2000 --action_multiplier_inc 0.2 --action_multiplier_dec 0.2 --agent_features action qlength_reward --target 0.005 --factor 80 --max_factor 0.01  --rollout_length 5 --action_loss_coeff 1 --loss_scale 1 --scenarios 2_1_qp_t 4_1_qp_t 8_1_qp_t 4_16_a2a_qp_t 8_8_a2a_qp_t 8_16_a2a_qp_t 32_1_qp_t  --limit_flows --history_length 2 --leaky_relu_coeff 0.1 --warmup_updates 50 --warmup_length 1024 --max_batch_size 4000 --discount 0.99 --max_step_size 4000 --port_increment 50
python3 run.py --envs_per_scenario 1 --wandb rlcc-new_simulator_v3 --wandb_run_name tele_12_12_h_tgt_0.001_max_0.01_f_1000_1_rl_5_act_0.2_alc_1_sl_1_lr_0.01_envs_1_qp_4k_bst_2_hl_lrelu_0.1_wmpup_50_wmplen_1024_mbatch_8192_msz_4000_g_0.99 --learning_rate 0.01 --agent ADPG --config telemetry_adpg --architecture 12 12 --max_num_updates 2000 --action_multiplier_inc 0.2 --action_multiplier_dec 0.2 --agent_features action qlength_reward --target 0.001 --factor 1000 --max_factor 0.01  --rollout_length 5 --action_loss_coeff 1 --loss_scale 1 --scenarios 2_1_qp_t 4_1_qp_t 8_1_qp_t 4_16_a2a_qp_t 8_8_a2a_qp_t 8_16_a2a_qp_t 32_1_qp_t  --limit_flows --history_length 2 --leaky_relu_coeff 0.1 --warmup_updates 50 --warmup_length 1024 --max_batch_size 4000 --discount 0.99 --max_step_size 4000 --port_increment 50

python3 run.py --envs_per_scenario 1 --wandb rlcc-new_simulator_v3 --wandb_run_name tele_12_12_h_tgt_0.005_max_0.02_f_80_1_rl_5_act_0.2_alc_1_sl_1_lr_0.01_envs_1_qp_4k_bst_2_hl_lrelu_0.01_wmpup_50_wmplen_1024_mbatch_8192_msz_4000_g_0.99 --learning_rate 0.01 --agent ADPG --config telemetry_adpg --architecture 12 12 --max_num_updates 2000 --action_multiplier_inc 0.2 --action_multiplier_dec 0.2 --agent_features action qlength_reward --target 0.005 --factor 80 --max_factor 0.02  --rollout_length 5 --action_loss_coeff 1 --loss_scale 1 --scenarios 2_1_qp_t 4_1_qp_t 8_1_qp_t 4_16_a2a_qp_t 8_8_a2a_qp_t 8_16_a2a_qp_t 32_1_qp_t  --limit_flows --history_length 2 --leaky_relu_coeff 0.01 --warmup_updates 50 --warmup_length 1024 --max_batch_size 4000 --discount 0.99 --max_step_size 4000 --port_increment 50
python3 run.py --envs_per_scenario 1 --wandb rlcc-new_simulator_v3 --wandb_run_name tele_12_12_h_tgt_0.005_max_0.005_f_80_1_rl_5_act_0.2_alc_1_sl_1_lr_0.01_envs_1_qp_4k_bst_2_hl_lrelu_0.01_wmpup_50_wmplen_1024_mbatch_8192_msz_4000_g_0.99 --learning_rate 0.01 --agent ADPG --config telemetry_adpg --architecture 12 12 --max_num_updates 2000 --action_multiplier_inc 0.2 --action_multiplier_dec 0.2 --agent_features action qlength_reward --target 0.005 --factor 80 --max_factor 0.005  --rollout_length 5 --action_loss_coeff 1 --loss_scale 1 --scenarios 2_1_qp_t 4_1_qp_t 8_1_qp_t 4_16_a2a_qp_t 8_8_a2a_qp_t 8_16_a2a_qp_t 32_1_qp_t  --limit_flows --history_length 2 --leaky_relu_coeff 0.01 --warmup_updates 50 --warmup_length 1024 --max_batch_size 4000 --discount 0.99 --max_step_size 4000 --port_increment 50
# qp-ip mode with normalized update delay as input
python3 run.py --envs_per_scenario 1 --wandb rlcc-new_simulator_v3 --wandb_run_name ip_qp_rtt_12_12_h_tgt_0.064_max_1.5_f_15_rl_3_act_0.2_alc_1_sl_5_lr_0.01_envs_1_4k_bst_hl_2_lrelu_0.1_wmpup_50_wmplen_1024_mbatch_8192_msz_5000_max_no_2_1_64_1 --learning_rate 0.01 --agent ADPG --config rtt_adpg --architecture 12 12 --max_num_updates 1000 --action_multiplier_inc 0.2 --action_multiplier_dec 0.2 --agent_features action rtt_reward update_delay --target 0.064 --factor 15 --max_factor 1.5 --base_rtt 8192 --rollout_length 5 --action_loss_coeff 1 --loss_scale 5 --scenarios 4_1_qp 4_16_qp 8_1_qp  8_8_qp 4_16_a2a_qp 8_8_a2a_qp 8_16_a2a_qp 32_1_qp 64_1_qp 4_1024_ip 8_1024_ip 8_512_ip  --limit_flows --history_length 2 --leaky_relu_coeff 0.1 --warmup_updates 50 --warmup_length 1024  --max_batch_size 8192 --discount 0.99 --max_step_size 5000 --port_increment 50
python3 run.py --envs_per_scenario 1 --wandb rlcc-new_simulator_v3 --wandb_run_name ip_qp_rtt_12_12_h_tgt_0.064_max_1.5_f_15_rl_3_act_0.2_alc_1_sl_5_lr_0.01_envs_1_4k_bst_hl_2_lrelu_0.1_wmpup_50_wmplen_1024_mbatch_8192_msz_5000_max_no_2_1_64_1_normudly --learning_rate 0.01 --agent ADPG --config rtt_adpg --architecture 12 12 --max_num_updates 1000 --action_multiplier_inc 0.2 --action_multiplier_dec 0.2 --agent_features action rtt_reward norm_update_delay --target 0.064 --factor 15 --max_factor 1.5 --base_rtt 8192 --rollout_length 5 --action_loss_coeff 1 --loss_scale 5 --scenarios 4_1_qp 4_16_qp 8_1_qp  8_8_qp 4_16_a2a_qp 8_8_a2a_qp 8_16_a2a_qp 32_1_qp 64_1_qp 4_1024_ip 8_1024_ip 8_512_ip  --limit_flows --history_length 2 --leaky_relu_coeff 0.1 --warmup_updates 50 --warmup_length 1024  --max_batch_size 8192 --discount 0.99 --max_step_size 5000 --port_increment 50
python3 run.py --envs_per_scenario 1 --wandb rlcc-new_simulator_v3 --wandb_run_name ip_qp_rtt_12_12_h_tgt_0.064_max_1.5_f_15_rl_3_act_0.2_alc_1_sl_5_lr_0.01_envs_1_4k_bst_hl_2_lrelu_0.01_wmpup_50_wmplen_1024_mbatch_8192_msz_5000_max_no_2_1_64_1_normudly --learning_rate 0.01 --agent ADPG --config rtt_adpg --architecture 12 12 --max_num_updates 1000 --action_multiplier_inc 0.2 --action_multiplier_dec 0.2 --agent_features action rtt_reward norm_update_delay --target 0.064 --factor 15 --max_factor 1.5 --base_rtt 8192 --rollout_length 5 --action_loss_coeff 1 --loss_scale 5 --scenarios 4_1_qp 4_16_qp 8_1_qp  8_8_qp 4_16_a2a_qp 8_8_a2a_qp 8_16_a2a_qp 32_1_qp 64_1_qp 4_1024_ip 8_1024_ip 8_512_ip  --limit_flows --history_length 2 --leaky_relu_coeff 0.01 --warmup_updates 50 --warmup_length 1024  --max_batch_size 8192 --discount 0.99 --max_step_size 5000 --port_increment 50
python3 run.py --envs_per_scenario 1 --wandb rlcc-new_simulator_v3 --wandb_run_name ip_qp_rtt_12_12_h_tgt_0.064_max_1.5_f_15_rl_3_act_0.2_alc_1_sl_5_lr_0.01_envs_1_4k_bst_hl_2_lrelu_0.01_wmpup_50_wmplen_1024_mbatch_8192_msz_5000_max_no_2_1_64_1_normudly_gradclip --learning_rate 0.01 --agent ADPG --config rtt_adpg --architecture 12 12 --max_num_updates 1000 --action_multiplier_inc 0.2 --action_multiplier_dec 0.2 --agent_features action rtt_reward norm_update_delay --target 0.064 --factor 15 --max_factor 1.5 --base_rtt 8192 --rollout_length 5 --action_loss_coeff 1 --loss_scale 5 --scenarios 4_1_qp 4_16_qp 8_1_qp  8_8_qp 4_16_a2a_qp 8_8_a2a_qp 8_16_a2a_qp 32_1_qp 64_1_qp 4_1024_ip 8_1024_ip 8_512_ip  --limit_flows --history_length 2 --leaky_relu_coeff 0.01 --warmup_updates 50 --warmup_length 1024  --max_batch_size 8192 --discount 0.99 --max_step_size 5000 --port_increment 50
python3 run.py --envs_per_scenario 1 --wandb rlcc-new_simulator_v3 --wandb_run_name ip_qp_rtt_12_12_h_tgt_0.064_max_1.5_f_15_rl_3_act_0.2_alc_1_sl_5_lr_0.01_envs_1_4k_bst_hl_2_lrelu_0.01_wmpup_50_wmplen_1024_mbatch_8192_msz_5000_max_no_2_1_64_1_binary_gradclip --learning_rate 0.01 --agent ADPG --config rtt_adpg --architecture 12 12 --max_num_updates 1000 --action_multiplier_inc 0.2 --action_multiplier_dec 0.2 --agent_features action rtt_reward --target 0.064 --factor 15 --max_factor 1.5 --base_rtt 8192 --rollout_length 5 --action_loss_coeff 1 --loss_scale 5 --scenarios 4_1_qp 4_16_qp 8_1_qp  8_8_qp 4_16_a2a_qp 8_8_a2a_qp 8_16_a2a_qp 32_1_qp 64_1_qp 4_1024_ip 8_1024_ip 8_512_ip  --limit_flows --history_length 2 --leaky_relu_coeff 0.01 --warmup_updates 50 --warmup_length 1024  --max_batch_size 8192 --discount 0.99 --max_step_size 5000 --port_increment 50
python3 run.py --envs_per_scenario 1 --wandb rlcc-new_simulator_v3 --wandb_run_name ip_qp_rtt_12_12_h_tgt_0.064_max_1.5_f_15_rl_3_act_0.2_alc_1_sl_5_lr_0.01_envs_1_4k_bst_hl_2_lrelu_0.01_wmpup_50_wmplen_1024_mbatch_8192_msz_5000_max_no_2_1_64_1_update_delay_gradclip --learning_rate 0.01 --agent ADPG --config rtt_adpg --architecture 12 12 --max_num_updates 1000 --action_multiplier_inc 0.2 --action_multiplier_dec 0.2 --agent_features action rtt_reward update_delay --target 0.064 --factor 15 --max_factor 1.5 --base_rtt 8192 --rollout_length 5 --action_loss_coeff 1 --loss_scale 5 --scenarios 4_1_qp 4_16_qp 8_1_qp  8_8_qp 4_16_a2a_qp 8_8_a2a_qp 8_16_a2a_qp 32_1_qp 64_1_qp 4_1024_ip 8_1024_ip 8_512_ip  --limit_flows --history_length 2 --leaky_relu_coeff 0.01 --warmup_updates 50 --warmup_length 1024  --max_batch_size 8192 --discount 0.99 --max_step_size 5000 --port_increment 50
python3 run.py --envs_per_scenario 1 --wandb rlcc-new_simulator_v3 --wandb_run_name ip_qp_rtt_12_12_h_tgt_0.064_max_1.5_f_15_rl_3_act_0.2_alc_1_sl_5_lr_0.01_envs_1_4k_bst_hl_2_lrelu_0.01_wmpup_50_wmplen_1024_mbatch_8192_msz_5000_max_no_2_1_64_1_update_delay_gradclip_notanh --learning_rate 0.01 --agent ADPG --config rtt_adpg --architecture 12 12 --max_num_updates 1000 --action_multiplier_inc 0.2 --action_multiplier_dec 0.2 --agent_features action rtt_reward update_delay --target 0.064 --factor 15 --max_factor 1.5 --base_rtt 8192 --rollout_length 5 --action_loss_coeff 1 --loss_scale 5 --scenarios 4_1_qp 4_16_qp 8_1_qp  8_8_qp 4_16_a2a_qp 8_8_a2a_qp 8_16_a2a_qp 32_1_qp 64_1_qp 4_1024_ip 8_1024_ip 8_512_ip  --limit_flows --history_length 2 --leaky_relu_coeff 0.01 --warmup_updates 50 --warmup_length 1024  --max_batch_size 8192 --discount 0.99 --max_step_size 5000 --port_increment 50

python3 run.py --envs_per_scenario 1 --wandb rlcc-new_simulator_v3 --wandb_run_name ip_qp_rtt_12_12_h_tgt_0.064_max_1.5_f_15_rl_3_act_0.2_alc_1_sl_5_lr_0.01_envs_1_4k_bst_hl_2_lrelu_0.01_wmpup_50_wmplen_1024_mbatch_8192_msz_5000_max_no_2_1_64_1_update_delay_gradclip_with_tanh --learning_rate 0.01 --agent ADPG --config rtt_adpg --architecture 12 12 --max_num_updates 1000 --action_multiplier_inc 0.2 --action_multiplier_dec 0.2 --agent_features action rtt_reward update_delay --target 0.064 --factor 15 --max_factor 1.5 --base_rtt 8192 --rollout_length 5 --action_loss_coeff 1 --loss_scale 5 --scenarios 4_1_qp 4_16_qp 8_1_qp  8_8_qp 4_16_a2a_qp 8_8_a2a_qp 8_16_a2a_qp 32_1_qp 64_1_qp 4_1024_ip 8_1024_ip 8_512_ip  --limit_flows --history_length 2 --leaky_relu_coeff 0.01 --warmup_updates 50 --warmup_length 1024  --max_batch_size 8192 --discount 0.99 --max_step_size 5000 --port_increment 50
# ip mode alone
python3 run.py --envs_per_scenario 1 --wandb rlcc-new_simulator_v3 --wandb_run_name ip_rtt_12_12_h_tgt_0.064_max_1.5_f_15_rl_3_act_0.2_alc_1_sl_2.5_lr_0.01_envs_1_qp_4k_bst_hl_2_lrelu_0.01_wmpup_50_wmplen_1024_mbatch_8192_msz_10000_max --learning_rate 0.01 --agent ADPG --config rtt_adpg --architecture 12 12 --max_num_updates 1000 --action_multiplier_inc 0.2 --action_multiplier_dec 0.2 --agent_features action rtt_reward --target 0.064 --factor 15 --max_factor 1.5 --base_rtt 8192 --rollout_length 5 --action_loss_coeff 1 --loss_scale 2.5 --scenarios 4_1_ip 4_16_ip 8_1_ip  8_8_ip 4_16_a2a_ip 8_8_a2a_ip 8_16_a2a_ip 32_1_ip 64_1_ip 64_128_ip 4_512_ip 4_1024_ip 8_1024_ip --limit_flows --history_length 2 --leaky_relu_coeff 0.01 --warmup_updates 50 --warmup_length 1024  --max_batch_size 8192 --discount 0.99 --max_step_size 10000 --port_increment 50
python3 run.py --envs_per_scenario 1 --wandb rlcc-new_simulator_v3 --wandb_run_name ip_rtt_12_12_h_tgt_0.064_max_1.5_f_15_rl_3_act_0.2_alc_1_sl_2.5_lr_0.01_envs_1_qp_4k_bst_hl_2_lrelu_0.01_wmpup_50_wmplen_1024_mbatch_8192_msz_10000_max_lmt_flows --learning_rate 0.01 --agent ADPG --config rtt_adpg --architecture 12 12 --max_num_updates 1000 --action_multiplier_inc 0.2 --action_multiplier_dec 0.2 --agent_features action rtt_reward --target 0.064 --factor 15 --max_factor 1.5 --base_rtt 8192 --rollout_length 5 --action_loss_coeff 1 --loss_scale 2.5 --scenarios 4_1_ip 4_16_ip 8_1_ip  8_8_ip 4_16_a2a_ip 8_8_a2a_ip 8_16_a2a_ip 32_1_ip 64_1_ip 64_128_ip 4_512_ip 4_1024_ip 8_1024_ip --limit_flows --history_length 2 --leaky_relu_coeff 0.01 --warmup_updates 50 --warmup_length 1024  --max_batch_size 8192 --discount 0.99 --max_step_size 10000 --limit_flows --port_increment 50
python3 run.py --envs_per_scenario 1 --wandb rlcc-new_simulator_v3 --wandb_run_name ip_rtt_12_12_h_tgt_0.064_max_1.5_f_15_rl_3_act_0.2_alc_1_sl_1_lr_0.01_envs_1_qp_4k_bst_hl_2_lrelu_0.01_wmpup_50_wmplen_1024_mbatch_8192_msz_10000_max_lmt_flows --learning_rate 0.01 --agent ADPG --config rtt_adpg --architecture 12 12 --max_num_updates 1000 --action_multiplier_inc 0.2 --action_multiplier_dec 0.2 --agent_features action rtt_reward --target 0.064 --factor 15 --max_factor 1.5 --base_rtt 8192 --rollout_length 5 --action_loss_coeff 1 --loss_scale 1 --scenarios 4_1_ip 4_16_ip 8_1_ip  8_8_ip 4_16_a2a_ip 8_8_a2a_ip 8_16_a2a_ip 32_1_ip 64_1_ip 64_128_ip 4_512_ip 4_1024_ip 8_1024_ip --limit_flows --history_length 2 --leaky_relu_coeff 0.01 --warmup_updates 50 --warmup_length 1024  --max_batch_size 8192 --discount 0.99 --max_step_size 10000 --limit_flows --port_increment 50
python3 run.py --envs_per_scenario 1 --wandb rlcc-new_simulator_v3 --wandb_run_name ip_rtt_12_12_h_tgt_0.064_max_1.5_f_15_rl_3_act_0.2_alc_1_sl_1_lr_0.01_envs_1_qp_4k_bst_hl_2_lrelu_0.01_wmpup_50_wmplen_1024_mbatch_8192_msz_10000_disct_0.95 --learning_rate 0.01 --agent ADPG --config rtt_adpg --architecture 12 12 --max_num_updates 1000 --action_multiplier_inc 0.2 --action_multiplier_dec 0.2 --agent_features action rtt_reward --target 0.064 --factor 15 --max_factor 1.5 --base_rtt 8192 --rollout_length 5 --action_loss_coeff 1 --loss_scale 1 --scenarios 4_1_ip 4_16_ip 8_1_ip  8_8_ip 4_16_a2a_ip 8_8_a2a_ip 8_16_a2a_ip 32_1_ip 64_1_ip 64_128_ip 4_512_ip 4_1024_ip 8_1024_ip --limit_flows --history_length 2 --leaky_relu_coeff 0.01 --warmup_updates 50 --warmup_length 1024  --max_batch_size 8192 --discount 0.95 --max_step_size 10000 --limit_flows --port_increment 50

# ip mode alone with update_delay
python3 run.py --envs_per_scenario 1 --wandb rlcc-new_simulator_v3 --wandb_run_name ip_rtt_12_12_h_tgt_0.064_max_1.5_f_15_rl_3_act_0.2_alc_1_sl_1_lr_0.01_envs_1_qp_4k_bst_hl_2_lrelu_0.01_wmpup_50_wmplen_1024_mbatch_8192_msz_10000_disct_0.95_updelay --learning_rate 0.01 --agent ADPG --config rtt_adpg --architecture 12 12 --max_num_updates 1000 --action_multiplier_inc 0.2 --action_multiplier_dec 0.2 --agent_features action rtt_reward update_delay --target 0.064 --factor 15 --max_factor 1.5 --base_rtt 8192 --rollout_length 5 --action_loss_coeff 1 --loss_scale 1 --scenarios 4_1_ip 4_16_ip 8_1_ip  8_8_ip 4_16_a2a_ip 8_8_a2a_ip 8_16_a2a_ip 32_1_ip 64_1_ip 64_128_ip 4_512_ip 4_1024_ip 8_1024_ip --limit_flows --history_length 2 --leaky_relu_coeff 0.01 --warmup_updates 50 --warmup_length 1024  --max_batch_size 8192 --discount 0.95 --max_step_size 10000 --limit_flows --port_increment 50
python3 run.py --envs_per_scenario 1 --wandb rlcc-new_simulator_v3 --wandb_run_name ip_rtt_12_12_h_tgt_0.064_max_1.5_f_15_rl_3_act_0.2_alc_1_sl_1_lr_0.01_envs_1_qp_4k_bst_hl_2_lrelu_0.01_wmpup_50_wmplen_1024_mbatch_8192_msz_10000_disct_1_updelay --learning_rate 0.01 --agent ADPG --config rtt_adpg --architecture 12 12 --max_num_updates 4000 --action_multiplier_inc 0.2 --action_multiplier_dec 0.2 --agent_features action rtt_reward update_delay --target 0.064 --factor 15 --max_factor 1.5 --base_rtt 8192 --rollout_length 5 --action_loss_coeff 1 --loss_scale 1 --scenarios 4_1_ip 4_16_ip 8_1_ip  8_8_ip 4_16_a2a_ip 8_8_a2a_ip 8_16_a2a_ip 32_1_ip 64_1_ip 64_128_ip 4_512_ip 4_1024_ip 8_1024_ip --limit_flows --history_length 2 --leaky_relu_coeff 0.01 --warmup_updates 50 --warmup_length 1024  --max_batch_size 8192 --discount 1 --max_step_size 10000 --limit_flows --port_increment 50
python3 run.py --envs_per_scenario 10 --wandb rlcc-new_simulator_v3 --wandb_run_name ip_rtt_12_12_h_tgt_0.064_max_1.5_f_15_rl_3_act_0.2_alc_1_sl_1_lr_0.01_envs_1_qp_4k_bst_hl_2_lrelu_0.01_wmpup_50_wmplen_1024_mbatch_8192_msz_10000_disct_1_only_8_1024 --learning_rate 0.01 --agent ADPG --config rtt_adpg --architecture 12 12 --max_num_updates 4000 --action_multiplier_inc 0.2 --action_multiplier_dec 0.2 --agent_features action rtt_reward --target 0.064 --factor 15 --max_factor 1.5 --base_rtt 8192 --rollout_length 5 --action_loss_coeff 1 --loss_scale 1 --scenarios 8_1024_ip --limit_flows --history_length 2 --leaky_relu 0.01 --warmup_updates 50 --warmup_length 1024  --max_batch_size 8192 --discount 1 --max_step_size 10000 --limit_flows --port_increment 50

# added rate2delay fix
python3 run.py --envs_per_scenario 1 --wandb rlcc-new_simulator_v3 --wandb_run_name ip_rtt_12_12_h_tgt_0.064_max_1.5_f_15_rl_3_act_0.2_alc_1_sl_1_lr_0.01_envs_1_qp_4k_hl_2_lrelu_0.01_wmp_50_wmpl_1024_mbatch_8192_msz_10000_gamma_0.99 --learning_rate 0.01 --agent ADPG --config rtt_adpg --architecture 12 12 --max_num_updates 4000 --action_multiplier_inc 0.2 --action_multiplier_dec 0.2 --agent_features action rtt_reward --target 0.064 --factor 15 --max_factor 1.5 --base_rtt 8192 --rollout_length 5 --action_loss_coeff 1 --loss_scale 1 --scenarios 4_1_ip 4_16_ip 8_1_ip  8_8_ip 4_16_a2a_ip 8_8_a2a_ip 8_16_a2a_ip 32_1_ip 64_1_ip 64_128_ip 4_512_ip 4_1024_ip 8_1024_ip --limit_flows --history_length 2 --leaky_relu_coeff 0.01 --warmup_updates 50 --warmup_length 1024  --max_batch_size 8192 --discount 0.99 --max_step_size 10000 --limit_flows --port_increment 50

# seems that it works well but reaction is too slow
python3 run.py --envs_per_scenario 1 --wandb rlcc-new_simulator_v3 --wandb_run_name ip_rtt_12_12_h_tgt_0.064_max_1.5_f_15_rl_3_act_0.2_alc_1_sl_5_lr_0.01_envs_1_qp_4k_hl_2_lrelu_0.01_wmp_50_wmpl_1024_mbatch_8192_msz_10000_gamma_0.99 --learning_rate 0.01 --agent ADPG --config rtt_adpg --architecture 12 12 --max_num_updates 4000 --action_multiplier_inc 0.2 --action_multiplier_dec 0.2 --agent_features action rtt_reward --target 0.064 --factor 15 --max_factor 1.5 --base_rtt 8192 --rollout_length 5 --action_loss_coeff 1 --loss_scale 5 --scenarios 4_1_ip 4_16_ip 8_1_ip  8_8_ip 4_16_a2a_ip 8_8_a2a_ip 8_16_a2a_ip 32_1_ip 64_1_ip 64_128_ip 4_512_ip 4_1024_ip 8_1024_ip --limit_flows --history_length 2 --leaky_relu_coeff 0.01 --warmup_updates 50 --warmup_length 1024  --max_batch_size 8192 --discount 0.99 --max_step_size 10000 --limit_flows --port_increment 50
python3 run.py --envs_per_scenario 1 --wandb rlcc-new_simulator_v3 --wandb_run_name ip_rtt_12_12_h_tgt_0.064_max_1.5_f_25_rl_3_act_0.2_alc_1_sl_5_lr_0.01_envs_1_qp_4k_hl_2_lrelu_0.01_wmp_50_wmpl_1024_mbatch_8192_msz_10000_gamma_0.99 --learning_rate 0.01 --agent ADPG --config rtt_adpg --architecture 12 12 --max_num_updates 4000 --action_multiplier_inc 0.2 --action_multiplier_dec 0.2 --agent_features action rtt_reward --target 0.064 --factor 25 --max_factor 1.5 --base_rtt 8192 --rollout_length 5 --action_loss_coeff 1 --loss_scale 5 --scenarios 4_1_ip 4_16_ip 8_1_ip  8_8_ip 4_16_a2a_ip 8_8_a2a_ip 8_16_a2a_ip 32_1_ip 64_1_ip 64_128_ip 4_512_ip 4_1024_ip 8_1024_ip --limit_flows --history_length 2 --leaky_relu_coeff 0.01 --warmup_updates 50 --warmup_length 1024  --max_batch_size 8192 --discount 0.99 --max_step_size 10000 --limit_flows --port_increment 50


python3 run.py --envs_per_scenario 1 --wandb rlcc-new_simulator_v3 --wandb_run_name sanity_check --learning_rate 0.01 --agent ADPG --config rtt_adpg --architecture 12 12 --max_num_updates 1000 --action_multiplier_inc 0.2 --action_multiplier_dec 0.2 --agent_features action rtt_reward --target 0.064 --factor 15 --max_factor 1.5 --base_rtt 8192 --rollout_length 5 --action_loss_coeff 1 --loss_scale 5 --scenarios 4_1_qp 4_16_qp 8_1_qp 8_8_qp 4_16_a2a_qp 8_8_a2a_qp 8_16_a2a_qp 32_1_qp 64_1_qp  --limit_flows --history_length 2 --leaky_relu_coeff 0.0 --warmup_updates 50 --warmup_length 1024  --max_batch_size 8192 --discount 0.99 --max_step_size 2000 --port_increment 50